\chapter{Foundations}
\label{foundations}
\section{Information Retrieval - Ranking Text}
Ranking is an integral part of the information retrieval (IR) process. The general IR problem can be formulated as follows: A user with a need for information expresses this information need through formulation of a query. Now given the query and a collection of documents, the IR system's task is to provide a list of documents that satisfy the user's information need. Further, the retrieved documents should be sorted by relevance w.r.t. the user's information need in descending order, i.e. the documents considered most relevant should be at the top of the list.

While from this formulation only, the task might appear simple, there are several caveats to look out for when it comes to ranking. For instance, there is no restriction on the structure of the query. While we might expect a natural language question like "What color are giraffes?" a user might decide to enter a keyword query like "giraffes color". The same applies to documents: Depending on the corpus we are dealing with, the documents might be raw text, structured text like HTML or even another type of media e.g. image, audio or a combination thereof.

Another possible issue is a mismatch in information need of the user and the corresponding query. Even if we find a perfect ordering of documents with respect to the query, we can not know for certain that the query actually reflects the user's information need. The user might not even know exactly what they're looking for until discovery through an iterative process, i.e. the information need is fuzzy and can not be specified through an exact query from the beginning on.

Further, a query might require additional context information in order for an IR system to find relevant documents. For example, depending on the time at which a query is prompted, the correct answer might change: "Who is president?" should return a different set of documents, as soon as a new president has been elected. Also, since not specified further, it is up to interpretation which country's president the user is interested in and might depend on their location.
In addition, even the corpus might not be static either and change or grow over time, e.g. web search has to deal with an ever-growing corpus: the internet.

While this list of issues is not comprehensive, at this point the complexity of the ranking problem should have become apparent.

Because this work focuses on the ranking of text in the context of web search, we will now give a formal definition with that scenario in mind:

Given a set of $|Q|$ natural language queries $Q = \{q_i\}_{i=1}^{|Q|}$ and a collection of $|D|$ documents $D = \{d_i\}_{i=1}^{|D|}$, we want to find a scoring function $s: Q \times D \rightarrow \mathbb{R}$, such that for any query $q \in Q$ and documents $d, d' \in D$, it holds true that $s(q, d) > s(q, d')$ if $d$ is more relevant w.r.t $q$ than $d'$.

To give the reader a more concrete idea and as we are going to build upon it throughout this work, we will now discuss two traditional approaches to text retrieval which, unlike neural retrieval, are based on exact matching, meaning query and document terms are compared directly. Further, they're "bag of word" models, meaning queries and documents are treated as sets of terms without considering order.

\subsection{TF-IDF}
Term Frequency - Inverted Document Frequency weighting (TF-IDF), is a traditional ranking approach that, given a query, assigns a relevance score to each document based on two assumptions:
\begin{enumerate}
    \item A document is relevant if terms from the query appear in it often.
    \item A document is relevant if the terms shared with the query are also rare in the collection.
\end{enumerate}

From these assumptions, two metrics are derived:
\begin{enumerate}
    \item Term-Frequency
          \begin{equation}
              w_{t,d} = \begin{cases}
                  1 + \log \text{tf}_{t, d} & \text{if } \text{tf}_{t, d} > 0 \\
                  0                         & \text{otherwise}                \\
              \end{cases}
          \end{equation}

          where $\text{tf}_{t, d}$ is the count of term $t$ in document $d$. The logarithmic scaling is motivated by the idea that a document does not linearly become more relevant by the number of terms in it: A document containing a term 10 times more often doesn't necessarily mean it is $10$ times more important, e.g. the document might just be very long and contain more words in general. Note that this is just one possible normalization scheme out of many.

    \item Inverted Document Frequency
          \begin{equation}
              \text{idf}(t, d) = \log \frac{|D|}{\text{df}_t}
          \end{equation}
          where $\text{df}_t$ counts the number of documents that a term occurs in over the full corpus. This way, terms that occur less frequent relative to the corpus size will receive a high IDF score and those that are more frequent a lower score.

\end{enumerate}

\begin{table}

\end{table}
To compute TF-IDF we can simply sum over the product of TF and IDF for each term in the query to produce a relevance score:
\begin{equation}
    \text{score}(q, d) = \sum_{t \in q} w_{t,d} \times \text{idf}_t
\end{equation}

Alternatively, vector space idf vector
\subsection{BM25}

\section{Machine Learning}
Machine learning can be described as a set of statistical methods, for automatically recognizing and extracting patterns from data. Typically, we can distinguish between two main types of machine learning: Supervised learning and unsupervised learning.

In the case of supervised learning, we have a set of training instances $X = \{x_i\}_{i=0}^N$ and corresponding labels $Y = \{y_i\}_{i=0}^N$, assigning a certain characteristic to each data point. For example, this characteristic might be a probability distribution over a set of classes or a regression score. Now given the training data and labels, the goal is to find a hypothesis that explains the data, such that for unseen data points $x' \notin X$, the labels $y' \notin Y$ can be inferred automatically. If each $y_i$ represents one or more categories from a fixed set of classes $C = \{c_i\}_{i=1}^{|C|}$, this is called a classification problem.

In contrast, in unsupervised learning there is no access to any labels whatsoever. Characteristics of the data need to be learned solely from the data $X$ itself. Examples for this include clustering where $X$ is clustered into groups, representation learning which usually tries to find vector representations for $X$, as well as dimensionality reduction that, if each $X$ is already a vector, tries to compress them into more compact but still informative representations.

That being said, the separating lines between supervised and unsupervised learning are blurry. Especially with the emergence of semi-supervised approaches and "end2end" representation learning, modern ML methods often integrate parts of both.

\section{Deep Learning}
Deep learning is a subfield of ML that makes use of a class of models called Deep Neural Networks (DNN). Typically, DNNs find application in the supervised learning scenario and are often used for classification tasks. In the following we explain the basic mechanisms of DNNs and common approaches to train them.

\subsection{Deep Neural Networks}
In essence, a Deep Neural Network (DNN) is a function approximator $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$ that applies a series of non-linear transformations to its inputs, in order to produce an output. In its simplest form, an input vector $x \in \mathbb{R}^n$ is multiplied by a single weight matrix, a bias vector is added, and the resulting vector is passed through a non-linear activation function $\sigma$.

\begin{equation}
    f(x) = \sigma(W x + b)
\end{equation}

where $W \in \mathbb{R}^{m \times n}$ and $b \in \mathbb{R}^{m}$ are learnable parameters.
This model is commonly referred to as single layer feed-forward neural network (FFN) or single layer perceptron.

When used for classification, a single layer FFN is limited to problems that require linear separation. In order to learn more complex, non-linear decision boundaries, multiple layers can be applied in sequence. 

An $L$-layer DNN can be described as follows:

\begin{equation}
    \begin{split}
        h^{(1)} &= \sigma^{(1)}(W^{(1)} x + b^{(1)}) \\
        h^{(2)} &= \sigma^{(2)}(W^{(2)} h^{(1)} + b^{(2)}) \\
        & \vdots \\
        f(x) &= \sigma^{(L)}(W^{(L)} h^{(L-1)} + b^{(L)})
    \end{split}
\end{equation}


\subsection{Optimization}
Arguably, the most common way for optimizing a neural network are the gradient descent (GD) algorithm and its variants. For this, an objective function $J(\theta)$ is defined, based on the DNN's outputs and corresponding target labels over the training set.

\begin{equation}
    J(\theta) = \frac{1}{N} \sum_{i=1}^{N} \mathcal{L}(f(x_i; \theta), y_i)
\end{equation}

Here, $\mathcal{L}$ is a differentiable loss function and $\theta$ represents the vector of all learnable parameters of the neural network $f(x)$.

\subsubsection{Gradient Descent}
For GD, the gradient of $J(\theta)$ with respect to $\theta$ is computed and scaled by a hyperparameter called learning rate $\eta$. If the objective is to minimize, the scaled gradient is subtracted from the original parameter vector.

\begin{equation}
    \theta_{new} = \theta - \eta\nabla_\theta J(\theta)
\end{equation}

By repeating this procedure iteratively, we can gradually minimize $J(\theta)$.

Common choices for $\mathcal{L}$ include:
\begin{itemize}
    \item \textbf{Cross Entropy Loss}
          \begin{equation}
              \text{CE}(y, \hat{y}) = - \sum_{k=1}^C y_k \log \hat{y}_k
          \end{equation}
          for classification tasks. Where $y_k$ is the ground truth probability of class $k$ and $\hat{y}_k$ the corresponding prediction.

    \item \textbf{Mean Squared Error}
          \begin{equation}
              \text{MSE}(y, \hat{y}) = (y - \hat{y})^2
          \end{equation}
          in the case of regression.
\end{itemize}

\subsubsection{Stochastic Gradient Descent}
The aforementioned algorithm is also known as the batch gradient descent (BGD) variant. Stochastic Gradient Descent (SGD) differs from BGD in the number of training samples that are used for a gradient update. Where BGD uses the gradient of the full training set for updating $\theta$, SGD only considers a single, randomly picked sample for each update. Not only can this approach be more efficient, since less redundant computations are performed, due to its stochastic nature and high variance, it is more likely to break out of local minima, allowing additional exploration for better solutions. \cite{ruder2016overview}

\subsubsection{Mini-Batch Gradient Descent}
While SGD's high variance during training makes it more likely to escape local minima, it can also come with the disadvantage of unstable training. In this scenario, convergence might be hindered by overshooting desirable minima.

To mitigate this issue, we can simply use more than $1$ sample, in order to achieve a more accurate estimate of the full gradient. Now, at each step a small subset of the dataset is sampled to reduce variance and stabilize training while retaining a level of stochasticity. This variant of gradient descent is called mini-batch gradient descent.

\subsubsection{Backpropagation}

\subsubsection{Momentum}
\subsubsection{Adam}

\subsection{Regularization}
\subsubsection{Weight Decay}
\subsubsection{Dropout}

\subsection{Learning to Rank}


\section{Transformer Models}
\subsection{Architecture}
\subsection{Pre-Training - BERT}

\section{Probing}
