\chapter{Introduction}
\section{Motivation}
Over the last couple of years, contextualized language representations from deep neural network (DNN) models have become the go-to approach for tackling natural language processing (NLP) tasks. In particular, the \ti{transformer} \citep{vaswani2017attention} and its variants, combined with large-scale, unsupervised pre-training, have shown unprecedented performance in a variety of NLP benchmarks.

However, unlike traditional approaches, these models consist of billions of automatically learned parameters, effectively turning them into huge black box functions \citep{Samek2021ExplainingDN}. Because of this, understanding why and how such a model arrives at a certain decision becomes a challenge in itself. Yet, as more and more NLP systems rely on these kinds of models, gaining a better understanding of their internal workings becomes crucial, especially when facing problems such as learned social biases \citep{Nadeem2021StereoSetMS,Bender2021OnTD, kurita2019measuring}, falsely motivated predictions \citep{10.1145/2939672.2939778, DBLP:journals/corr/abs-1802-00614} or simply for determining causes of prediction error. In addition, a better understanding might provide insights on model weaknesses and guide model improvement, e.g. when adapting a model to new domains.

Recent work on understanding neural language models has aimed at measuring the extent to which certain information is encoded in their word representations \citep{tenney-etal-2019-bert,Tenney2019WhatDY,DBLP:journals/corr/abs-1909-03368}. To achieve this, a \ti{probing} classifier or \ti{probe} is trained to predict certain linguistic properties from these representations. The probe's performance is then expected to reflect how well said property is captured. By employing multiple probing tasks, the presence of different types of information can be estimated. For example, by training a probe for part-of-speech tagging, it can be tested whether it is possible to extract part-of-speech information from the representations.

While the probing paradigm recently emerged for understanding how neural language models encode general linguistic properties, it has yet to be explored, as to what kind of information is stored regarding \ti{downstream tasks} when fine-tuning such models. In our particular case, we're interested in the task of \ti{text retrieval}. Until now, little work has been put into explicitly probing language models for ranking.

However, when considering how Information Retrieval already affects our daily lives, (e.g. web search, online shopping, searching streaming services) and that for this purpose, large language models are already widely in use\footnote{\url{https://blog.google/products/search/search-language-understanding-bert/}}\footnote{\url{https://searchengineland.com/google-bert-used-on-almost-every-english-query-342193}}, getting a better understanding of these models is undoubtedly important. For this reason, we want to shed light on the inner workings of the \ti{BERT} \citep{devlin-etal-2019-bert} transformer model, in the context of ranking. We choose this model, as most recent neural IR approaches are built on BERT or its descendants, i.e. improved or specialized variants that usually share the same core architecture.

\section{Problem Statement}
Given a pre-trained BERT language model, we want to find out as to what extent its hidden representations capture commonly known ranking properties. We further want to know how this kind of information is distributed across different layers of the model and whether these insights help us build a better ranking model. Our research questions are as follows:
\begin{itemize}
    \item Does BERT encode known ranking properties?
    \item If this is the case, which layers capture which property?
    \item Does the distribution change if we fine-tune the model for ranking?
    \item Can we improve BERT's ranking abilities based on those findings?
\end{itemize}

\section{Contribution}
This thesis is structured into two parts: Firstly, we apply the probing paradigm in order to understand how a BERT model captures certain ranking properties. We perform extensive analysis and evaluate how ranking related information is distributed throughout the model.

Secondly, we leverage the insights gained from our probing experiments by designing a multi-task learning setup for BERT. We show that infusing additional task information can result in improved ranking performance when compared to a standard fine-tuning approach.

\section{Thesis Outline}
\begin{itemize}
    \item In the next chapter, we provide the foundations needed for this thesis. We briefly discuss traditional ranking approaches (\Cref{sec:ir}), then provide the basic ideas behind machine learning and deep learning (\Cref{sec:ml}, \Cref{sec:dl}) and explain the transformer model (\Cref{sec:transformer}), which is the main subject of our study.
    \item \Cref{chap:prev} presents previous work done in the fields of neural IR, probing and multi-task learning which is related to this thesis.
    \item In \Cref{chap:datasets} we present the datasets that were used in this thesis.
    \item \Cref{chap:approach} explains our approach to probing, including task design (\Cref{sec:tasks}), the experimental setup (\Cref{sec:probing_setup}) as well as the evaluation measures (\Cref{sec:metrics}) that were used.
    \item In \Cref{chap:results} we present the results from the probing experiments and provide an in-depth analysis.
    \item \Cref{chap:mtl} builds upon the findings from \Cref{chap:results}. We introduce a novel multi-task learning (MTL) setup, to aid the fine-tuning of BERT for ranking. Again, we present our results and provide analysis (\Cref{sec:mtl_results}). Furthermore, we perform an ablation study that investigates how this MTL setup behaves in a low resource scenario (\Cref{sec:ablation}).
    \item In \Cref{chap:conclusion} we conclude this thesis, discuss limitations of our approach (\Cref{sec:limitations}) and give an outlook on potential future work (\Cref{sec:future})
\end{itemize}
