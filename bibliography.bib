@inproceedings{devlin-etal-2019-bert,
  title     = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author    = {Devlin, Jacob  and
               Chang, Ming-Wei  and
               Lee, Kenton  and
               Toutanova, Kristina},
  booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  month     = jun,
  year      = {2019},
  address   = {Minneapolis, Minnesota},
  publisher = {Association for Computational Linguistics},
  url       = {https://www.aclweb.org/anthology/N19-1423},
  pages     = {4171--4186},
  abstract  = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).}
}

@article{DBLP:journals/corr/NguyenRSGTMD16,
  author        = {Tri Nguyen and
                   Mir Rosenberg and
                   Xia Song and
                   Jianfeng Gao and
                   Saurabh Tiwary and
                   Rangan Majumder and
                   Li Deng},
  title         = {{MS} {MARCO:} {A} Human Generated MAchine Reading COmprehension Dataset},
  journal       = {CoRR},
  volume        = {abs/1611.09268},
  year          = {2016},
  url           = {http://arxiv.org/abs/1611.09268},
  archiveprefix = {arXiv},
  eprint        = {1611.09268},
  timestamp     = {Mon, 13 Aug 2018 16:49:03 +0200},
  biburl        = {https://dblp.org/rec/bib/journals/corr/NguyenRSGTMD16},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@article{robertson2009probabilistic,
  title     = {The probabilistic relevance framework: BM25 and beyond},
  author    = {Robertson, Stephen and Zaragoza, Hugo and others},
  journal   = {Foundations and Trends{\textregistered} in Information Retrieval},
  volume    = {3},
  number    = {4},
  pages     = {333--389},
  year      = {2009},
  publisher = {Now Publishers, Inc.}
}

@article{ruder2016overview,
  title   = {An overview of gradient descent optimization algorithms},
  author  = {Ruder, Sebastian},
  journal = {arXiv preprint arXiv:1609.04747},
  year    = {2016}
}

@article{rumelhart1988learning,
  title   = {Learning representations by back-propagating errors},
  author  = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J and others},
  journal = {Cognitive modeling},
  volume  = {5},
  number  = {3},
  pages   = {1},
  year    = {1988}
}

@inproceedings{nair2010rectified,
  title     = {Rectified linear units improve restricted boltzmann machines},
  author    = {Nair, Vinod and Hinton, Geoffrey E},
  booktitle = {Proceedings of the 27th international conference on machine learning (ICML-10)},
  pages     = {807--814},
  year      = {2010}
}

@article{DBLP:journals/corr/abs-1811-05544,
  author        = {Dichao Hu},
  title         = {An Introductory Survey on Attention Mechanisms in {NLP} Problems},
  journal       = {CoRR},
  volume        = {abs/1811.05544},
  year          = {2018},
  url           = {http://arxiv.org/abs/1811.05544},
  archiveprefix = {arXiv},
  eprint        = {1811.05544},
  timestamp     = {Sat, 24 Nov 2018 17:52:00 +0100},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1811-05544},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@article{bahdanau2014neural,
  title   = {Neural machine translation by jointly learning to align and translate},
  author  = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal = {arXiv preprint arXiv:1409.0473},
  year    = {2014}
}

@inproceedings{vaswani2017attention,
  title     = {Attention is all you need},
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle = {Advances in neural information processing systems},
  pages     = {5998--6008},
  year      = {2017}
}

@article{luong2015effective,
  title   = {Effective approaches to attention-based neural machine translation},
  author  = {Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D},
  journal = {arXiv preprint arXiv:1508.04025},
  year    = {2015}
}

@incollection{NIPS2013_5021,
  title     = {Distributed Representations of Words and Phrases and their Compositionality},
  author    = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  booktitle = {Advances in Neural Information Processing Systems 26},
  editor    = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
  pages     = {3111--3119},
  year      = {2013},
  publisher = {Curran Associates, Inc.},
  url       = {http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf}
}

@article{ba2016layer,
  title   = {Layer normalization},
  author  = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal = {arXiv preprint arXiv:1607.06450},
  year    = {2016}
}

@article{DBLP:journals/corr/IoffeS15,
  author        = {Sergey Ioffe and
                   Christian Szegedy},
  title         = {Batch Normalization: Accelerating Deep Network Training by Reducing
                   Internal Covariate Shift},
  journal       = {CoRR},
  volume        = {abs/1502.03167},
  year          = {2015},
  url           = {http://arxiv.org/abs/1502.03167},
  archiveprefix = {arXiv},
  eprint        = {1502.03167},
  timestamp     = {Mon, 13 Aug 2018 16:47:06 +0200},
  biburl        = {https://dblp.org/rec/bib/journals/corr/IoffeS15},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@article{manning2008irbook,
  title     = {Inroduction to Information Retrieval},
  author    = {Christopher D. Manning, Prabhakar Raghavan and Hinrich Schütze},
  publisher = {Cambridge University Press. 2008},
  year      = {2008},
  url       = {https://nlp.stanford.edu/IR-book/},
  publisher = {Now Publishers, Inc.}
}

@inproceedings{pennington2014glove,
  author    = {Jeffrey Pennington and Richard Socher and Christopher D. Manning},
  booktitle = {Empirical Methods in Natural Language Processing (EMNLP)},
  title     = {GloVe: Global Vectors for Word Representation},
  year      = {2014},
  pages     = {1532--1543},
  url       = {http://www.aclweb.org/anthology/D14-1162}
}

@article{hinton2012neural,
  title   = {Neural networks for machine learning lecture 6a overview of mini-batch gradient descent},
  journal = {University of Toronto, Technical Report},
  author  = {Hinton, Geoffrey and Srivastava, Nitish and Swersky, Kevin},
  year    = {2012}
}

@article{duchi2011adaptive,
  title   = {Adaptive subgradient methods for online learning and stochastic optimization},
  author  = {Duchi, John and Hazan, Elad and Singer, Yoram},
  journal = {Journal of Machine Learning Research},
  volume  = {12},
  number  = {Jul},
  pages   = {2121--2159},
  year    = {2011}
}

@article{kingma2014adam,
  title   = {Adam: A method for stochastic optimization},
  author  = {Kingma, Diederik P and Ba, Jimmy},
  journal = {arXiv preprint arXiv:1412.6980},
  year    = {2014}
}

@inproceedings{Peters:2018,
  author    = {Peters, Matthew E. and  Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  title     = {Deep contextualized word representations},
  booktitle = {Proc. of NAACL},
  year      = {2018}
}

@article{DBLP:journals/corr/KumarISBEPOGS15,
  author        = {Ankit Kumar and
                   Ozan Irsoy and
                   Jonathan Su and
                   James Bradbury and
                   Robert English and
                   Brian Pierce and
                   Peter Ondruska and
                   Ishaan Gulrajani and
                   Richard Socher},
  title         = {Ask Me Anything: Dynamic Memory Networks for Natural Language Processing},
  journal       = {CoRR},
  volume        = {abs/1506.07285},
  year          = {2015},
  url           = {http://arxiv.org/abs/1506.07285},
  archiveprefix = {arXiv},
  eprint        = {1506.07285},
  timestamp     = {Thu, 21 Mar 2019 11:19:44 +0100},
  biburl        = {https://dblp.org/rec/bib/journals/corr/KumarISBEPOGS15},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/WuSCLNMKCGMKSJL16,
  author        = {Yonghui Wu and
                   Mike Schuster and
                   Zhifeng Chen and
                   Quoc V. Le and
                   Mohammad Norouzi and
                   Wolfgang Macherey and
                   Maxim Krikun and
                   Yuan Cao and
                   Qin Gao and
                   Klaus Macherey and
                   Jeff Klingner and
                   Apurva Shah and
                   Melvin Johnson and
                   Xiaobing Liu and
                   Lukasz Kaiser and
                   Stephan Gouws and
                   Yoshikiyo Kato and
                   Taku Kudo and
                   Hideto Kazawa and
                   Keith Stevens and
                   George Kurian and
                   Nishant Patil and
                   Wei Wang and
                   Cliff Young and
                   Jason Smith and
                   Jason Riesa and
                   Alex Rudnick and
                   Oriol Vinyals and
                   Greg Corrado and
                   Macduff Hughes and
                   Jeffrey Dean},
  title         = {Google's Neural Machine Translation System: Bridging the Gap between
                   Human and Machine Translation},
  journal       = {CoRR},
  volume        = {abs/1609.08144},
  year          = {2016},
  url           = {http://arxiv.org/abs/1609.08144},
  archiveprefix = {arXiv},
  eprint        = {1609.08144},
  timestamp     = {Thu, 14 Mar 2019 09:34:18 +0100},
  biburl        = {https://dblp.org/rec/bib/journals/corr/WuSCLNMKCGMKSJL16},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@misc{tensorflow2015-whitepaper,
  title  = { {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
  url    = {https://www.tensorflow.org/},
  note   = {Software available from tensorflow.org},
  author = {
            Martìn~Abadi and
            Ashish~Agarwal and
            Paul~Barham and
            Eugene~Brevdo and
            Zhifeng~Chen and
            Craig~Citro and
            Greg~S.~Corrado and
            Andy~Davis and
            Jeffrey~Dean and
            Matthieu~Devin and
            Sanjay~Ghemawat and
            Ian~Goodfellow and
            Andrew~Harp and
            Geoffrey~Irving and
            Michael~Isard and
            Yangqing Jia and
            Rafal~Jozefowicz and
            Lukasz~Kaiser and
            Manjunath~Kudlur and
            Josh~Levenberg and
            Dandelion~Manè and
            Rajat~Monga and
            Sherry~Moore and
            Derek~Murray and
            Chris~Olah and
            Mike~Schuster and
            Jonathon~Shlens and
            Benoit~Steiner and
            Ilya~Sutskever and
            Kunal~Talwar and
            Paul~Tucker and
            Vincent~Vanhoucke and
            Vijay~Vasudevan and
            Fernanda~Viègas and
            Oriol~Vinyals and
            Pete~Warden and
            Martin~Wattenberg and
            Martin~Wicke and
            Yuan~Yu and
            Xiaoqiang~Zheng},
  year   = {2015}
}

@article{DBLP:journals/corr/abs-1901-04085,
  author        = {Rodrigo Nogueira and
                   Kyunghyun Cho},
  title         = {Passage Re-ranking with {BERT}},
  journal       = {CoRR},
  volume        = {abs/1901.04085},
  year          = {2019},
  url           = {http://arxiv.org/abs/1901.04085},
  archiveprefix = {arXiv},
  eprint        = {1901.04085},
  timestamp     = {Fri, 01 Feb 2019 13:39:59 +0100},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1901-04085},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1905-05910,
  author        = {Peng Xu and
                   Xiaofei Ma and
                   Ramesh Nallapati and
                   Bing Xiang},
  title         = {Passage Ranking with Weak Supervsion},
  journal       = {CoRR},
  volume        = {abs/1905.05910},
  year          = {2019},
  url           = {http://arxiv.org/abs/1905.05910},
  archiveprefix = {arXiv},
  eprint        = {1905.05910},
  timestamp     = {Tue, 28 May 2019 12:48:08 +0200},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1905-05910},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1804-07888,
  author        = {Xiaodong Liu and
                   Kevin Duh and
                   Jianfeng Gao},
  title         = {Stochastic Answer Networks for Natural Language Inference},
  journal       = {CoRR},
  volume        = {abs/1804.07888},
  year          = {2018},
  url           = {http://arxiv.org/abs/1804.07888},
  archiveprefix = {arXiv},
  eprint        = {1804.07888},
  timestamp     = {Mon, 13 Aug 2018 16:47:29 +0200},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1804-07888},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{burges2005learning,
  title     = {Learning to rank using gradient descent},
  author    = {Burges, Christopher and Shaked, Tal and Renshaw, Erin and Lazier, Ari and Deeds, Matt and Hamilton, Nicole and Hullender, Gregory N},
  booktitle = {Proceedings of the 22nd International Conference on Machine learning (ICML-05)},
  pages     = {89--96},
  year      = {2005}
}
@article{radford2018improving,
  title  = {Improving language understanding by generative pre-training},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year   = {2018}
}
@inproceedings{sukhbaatar2015end,
  title     = {End-to-end memory networks},
  author    = {Sukhbaatar, Sainbayar and Weston, Jason and Fergus, Rob and others},
  booktitle = {Advances in neural information processing systems},
  pages     = {2440--2448},
  year      = {2015}
}
@article{graves2014neural,
  title   = {Neural turing machines},
  author  = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
  journal = {arXiv preprint arXiv:1410.5401},
  year    = {2014}
}
@inproceedings{mitra2017learning,
  title        = {Learning to match using local and distributed representations of text for web search},
  author       = {Mitra, Bhaskar and Diaz, Fernando and Craswell, Nick},
  booktitle    = {Proceedings of the 26th International Conference on World Wide Web},
  pages        = {1291--1299},
  year         = {2017},
  organization = {International World Wide Web Conferences Steering Committee}
}
@inproceedings{dai2018convolutional,
  title        = {Convolutional neural networks for soft-matching n-grams in ad-hoc search},
  author       = {Dai, Zhuyun and Xiong, Chenyan and Callan, Jamie and Liu, Zhiyuan},
  booktitle    = {Proceedings of the eleventh ACM international conference on web search and data mining},
  pages        = {126--134},
  year         = {2018},
  organization = {ACM}
}
@book{Goodfellow-et-al-2016,
  title     = {Deep Learning},
  author    = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
  publisher = {MIT Press},
  note      = {\url{http://www.deeplearningbook.org}},
  year      = {2016}
}

@inproceedings{Tran:2018:NNF:3184558.3191830,
  author    = {Tran, Nam Khanh and Nieder{\'e}e, Claudia},
  title     = {A Neural Network-based Framework for Non-factoid Question Answering},
  booktitle = {Companion Proceedings of the The Web Conference 2018},
  series    = {WWW '18},
  year      = {2018},
  isbn      = {978-1-4503-5640-4},
  location  = {Lyon, France},
  pages     = {1979--1983},
  numpages  = {5},
  url       = {https://doi.org/10.1145/3184558.3191830},
  doi       = {10.1145/3184558.3191830},
  acmid     = {3191830},
  publisher = {International World Wide Web Conferences Steering Committee},
  address   = {Republic and Canton of Geneva, Switzerland},
  keywords  = {non-factoid question answering, representation learning}
} 

@inproceedings{zhu2015aligning,
  title     = {Aligning books and movies: Towards story-like visual explanations by watching movies and reading books},
  author    = {Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  booktitle = {Proceedings of the IEEE international conference on computer vision},
  pages     = {19--27},
  year      = {2015}
}

@article{mitra2018an,
  author   = {Mitra, Bhaskar and Craswell, Nick},
  title    = {An Introduction to Neural Information Retrieval},
  year     = {2018},
  abstract = {Neural ranking models for information retrieval (IR) use shallow or deep neural networks to rank search results in response to a query. Traditional learning to rank models employ supervised machine learning (ML) techniques—including neural networks—over hand-crafted IR features. By contrast, more recently proposed neural models learn representations of language from raw text that can bridge the gap between query and document vocabulary. Unlike classical learning to rank models and non-neural approaches to IR, these new ML techniques are data-hungry, requiring large scale training data before they can be deployed. This tutorial introduces basic concepts and intuitions behind neural IR models, and places them in the context of classical non-neural approaches to IR. We begin by introducing fundamental concepts of retrieval and different neural and non-neural approaches to unsupervised learning of vector representations of text. We then review IR methods that employ these pre-trained neural vector representations without learning the IR task end-to-end. We introduce the Learning to Rank (LTR) framework next, discussing standard loss functions for ranking. We follow that with an overview of deep neural networks (DNNs), including standard architectures and implementations. Finally, we review supervised neural learning to rank models, including recent DNN architectures trained end-to-end for ranking tasks. We conclude with a discussion on potential future directions for neural IR.},
  url      = {https://www.microsoft.com/en-us/research/publication/introduction-neural-information-retrieval/},
  pages    = {1-126},
  journal  = {Foundations and Trends® in Information Retrieval},
  volume   = {13},
  number   = {1}
}

@incollection{NEURIPS2019_9015,
  title     = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  author    = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  booktitle = {Advances in Neural Information Processing Systems 32},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages     = {8024--8035},
  year      = {2019},
  publisher = {Curran Associates, Inc.},
  url       = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@article{DBLP:journals/corr/abs-1207-0580,
  author     = {Geoffrey E. Hinton and
                Nitish Srivastava and
                Alex Krizhevsky and
                Ilya Sutskever and
                Ruslan Salakhutdinov},
  title      = {Improving neural networks by preventing co-adaptation of feature detectors},
  journal    = {CoRR},
  volume     = {abs/1207.0580},
  year       = {2012},
  url        = {http://arxiv.org/abs/1207.0580},
  eprinttype = {arXiv},
  eprint     = {1207.0580},
  timestamp  = {Mon, 13 Aug 2018 16:46:10 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1207-0580.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2010-11929,
  author     = {Alexey Dosovitskiy and
                Lucas Beyer and
                Alexander Kolesnikov and
                Dirk Weissenborn and
                Xiaohua Zhai and
                Thomas Unterthiner and
                Mostafa Dehghani and
                Matthias Minderer and
                Georg Heigold and
                Sylvain Gelly and
                Jakob Uszkoreit and
                Neil Houlsby},
  title      = {An Image is Worth 16x16 Words: Transformers for Image Recognition
                at Scale},
  journal    = {CoRR},
  volume     = {abs/2010.11929},
  year       = {2020},
  url        = {https://arxiv.org/abs/2010.11929},
  eprinttype = {arXiv},
  eprint     = {2010.11929},
  timestamp  = {Fri, 20 Nov 2020 14:04:05 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2010-11929.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@misc{https://doi.org/10.48550/arxiv.2005.00341,
  doi       = {10.48550/ARXIV.2005.00341},
  url       = {https://arxiv.org/abs/2005.00341},
  author    = {Dhariwal, Prafulla and Jun, Heewoo and Payne, Christine and Kim, Jong Wook and Radford, Alec and Sutskever, Ilya},
  keywords  = {Audio and Speech Processing (eess.AS), Machine Learning (cs.LG), Sound (cs.SD), Machine Learning (stat.ML), FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Jukebox: A Generative Model for Music},
  publisher = {arXiv},
  year      = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{DBLP:journals/corr/abs-1907-11692,
  author     = {Yinhan Liu and
                Myle Ott and
                Naman Goyal and
                Jingfei Du and
                Mandar Joshi and
                Danqi Chen and
                Omer Levy and
                Mike Lewis and
                Luke Zettlemoyer and
                Veselin Stoyanov},
  title      = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
  journal    = {CoRR},
  volume     = {abs/1907.11692},
  year       = {2019},
  url        = {http://arxiv.org/abs/1907.11692},
  eprinttype = {arXiv},
  eprint     = {1907.11692},
  timestamp  = {Thu, 01 Aug 2019 08:59:33 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2003-10555,
  author     = {Kevin Clark and
                Minh{-}Thang Luong and
                Quoc V. Le and
                Christopher D. Manning},
  title      = {{ELECTRA:} Pre-training Text Encoders as Discriminators Rather Than
                Generators},
  journal    = {CoRR},
  volume     = {abs/2003.10555},
  year       = {2020},
  url        = {https://arxiv.org/abs/2003.10555},
  eprinttype = {arXiv},
  eprint     = {2003.10555},
  timestamp  = {Wed, 01 Apr 2020 17:39:11 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2003-10555.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1909-08053,
  author     = {Mohammad Shoeybi and
                Mostofa Patwary and
                Raul Puri and
                Patrick LeGresley and
                Jared Casper and
                Bryan Catanzaro},
  title      = {Megatron-LM: Training Multi-Billion Parameter Language Models Using
                Model Parallelism},
  journal    = {CoRR},
  volume     = {abs/1909.08053},
  year       = {2019},
  url        = {http://arxiv.org/abs/1909.08053},
  eprinttype = {arXiv},
  eprint     = {1909.08053},
  timestamp  = {Tue, 24 Sep 2019 11:33:51 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1909-08053.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2005-14165,
  author     = {Tom B. Brown and
                Benjamin Mann and
                Nick Ryder and
                Melanie Subbiah and
                Jared Kaplan and
                Prafulla Dhariwal and
                Arvind Neelakantan and
                Pranav Shyam and
                Girish Sastry and
                Amanda Askell and
                Sandhini Agarwal and
                Ariel Herbert{-}Voss and
                Gretchen Krueger and
                Tom Henighan and
                Rewon Child and
                Aditya Ramesh and
                Daniel M. Ziegler and
                Jeffrey Wu and
                Clemens Winter and
                Christopher Hesse and
                Mark Chen and
                Eric Sigler and
                Mateusz Litwin and
                Scott Gray and
                Benjamin Chess and
                Jack Clark and
                Christopher Berner and
                Sam McCandlish and
                Alec Radford and
                Ilya Sutskever and
                Dario Amodei},
  title      = {Language Models are Few-Shot Learners},
  journal    = {CoRR},
  volume     = {abs/2005.14165},
  year       = {2020},
  url        = {https://arxiv.org/abs/2005.14165},
  eprinttype = {arXiv},
  eprint     = {2005.14165},
  timestamp  = {Wed, 03 Jun 2020 11:36:54 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2005-14165.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{xu2015show,
  title        = {Show, attend and tell: Neural image caption generation with visual attention},
  author       = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhudinov, Ruslan and Zemel, Rich and Bengio, Yoshua},
  booktitle    = {International conference on machine learning},
  pages        = {2048--2057},
  year         = {2015},
  organization = {PMLR}
}

@article{xiong2016dynamic,
  title   = {Dynamic coattention networks for question answering},
  author  = {Xiong, Caiming and Zhong, Victor and Socher, Richard},
  journal = {arXiv preprint arXiv:1611.01604},
  year    = {2016}
}


@article{DBLP:journals/corr/abs-1801-06146,
  author     = {Jeremy Howard and
                Sebastian Ruder},
  title      = {Fine-tuned Language Models for Text Classification},
  journal    = {CoRR},
  volume     = {abs/1801.06146},
  year       = {2018},
  url        = {http://arxiv.org/abs/1801.06146},
  eprinttype = {arXiv},
  eprint     = {1801.06146},
  timestamp  = {Mon, 13 Aug 2018 16:46:54 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1801-06146.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@inbook{10.5555/3454287.3454804,
author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V.},
title = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment setting, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {517},
numpages = {11}
}

@inproceedings{Lan2020ALBERT,
title={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
author={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=H1eA7AEtvS}
}