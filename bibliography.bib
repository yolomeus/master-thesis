@inproceedings{devlin-etal-2019-bert,
  title     = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author    = {Devlin, Jacob  and
               Chang, Ming-Wei  and
               Lee, Kenton  and
               Toutanova, Kristina},
  booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  month     = jun,
  year      = {2019},
  address   = {Minneapolis, Minnesota},
  publisher = {Association for Computational Linguistics},
  url       = {https://www.aclweb.org/anthology/N19-1423},
  pages     = {4171--4186},
  abstract  = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).}
}

@article{DBLP:journals/corr/NguyenRSGTMD16,
  author        = {Tri Nguyen and
                   Mir Rosenberg and
                   Xia Song and
                   Jianfeng Gao and
                   Saurabh Tiwary and
                   Rangan Majumder and
                   Li Deng},
  title         = {{MS} {MARCO:} {A} Human Generated MAchine Reading COmprehension Dataset},
  journal       = {CoRR},
  volume        = {abs/1611.09268},
  year          = {2016},
  url           = {http://arxiv.org/abs/1611.09268},
  archiveprefix = {arXiv},
  eprint        = {1611.09268},
  timestamp     = {Mon, 13 Aug 2018 16:49:03 +0200},
  biburl        = {https://dblp.org/rec/bib/journals/corr/NguyenRSGTMD16},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@article{robertson2009probabilistic,
  title     = {The probabilistic relevance framework: BM25 and beyond},
  author    = {Robertson, Stephen and Zaragoza, Hugo and others},
  journal   = {Foundations and Trends{\textregistered} in Information Retrieval},
  volume    = {3},
  number    = {4},
  pages     = {333--389},
  year      = {2009},
  publisher = {Now Publishers, Inc.}
}

@article{ruder2016overview,
  title   = {An overview of gradient descent optimization algorithms},
  author  = {Ruder, Sebastian},
  journal = {arXiv preprint arXiv:1609.04747},
  year    = {2016}
}

@article{rumelhart1988learning,
  title   = {Learning representations by back-propagating errors},
  author  = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J and others},
  journal = {Cognitive modeling},
  volume  = {5},
  number  = {3},
  pages   = {1},
  year    = {1988}
}

@inproceedings{nair2010rectified,
  title     = {Rectified linear units improve restricted boltzmann machines},
  author    = {Nair, Vinod and Hinton, Geoffrey E},
  booktitle = {Proceedings of the 27th international conference on machine learning (ICML-10)},
  pages     = {807--814},
  year      = {2010}
}

@article{DBLP:journals/corr/abs-1811-05544,
  author        = {Dichao Hu},
  title         = {An Introductory Survey on Attention Mechanisms in {NLP} Problems},
  journal       = {CoRR},
  volume        = {abs/1811.05544},
  year          = {2018},
  url           = {http://arxiv.org/abs/1811.05544},
  archiveprefix = {arXiv},
  eprint        = {1811.05544},
  timestamp     = {Sat, 24 Nov 2018 17:52:00 +0100},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1811-05544},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@article{bahdanau2014neural,
  title   = {Neural machine translation by jointly learning to align and translate},
  author  = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal = {arXiv preprint arXiv:1409.0473},
  year    = {2014}
}

@inproceedings{vaswani2017attention,
  title     = {Attention is all you need},
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle = {Advances in neural information processing systems},
  pages     = {5998--6008},
  year      = {2017}
}

@article{luong2015effective,
  title   = {Effective approaches to attention-based neural machine translation},
  author  = {Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D},
  journal = {arXiv preprint arXiv:1508.04025},
  year    = {2015}
}

@incollection{NIPS2013_5021,
  title     = {Distributed Representations of Words and Phrases and their Compositionality},
  author    = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  booktitle = {Advances in Neural Information Processing Systems 26},
  editor    = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
  pages     = {3111--3119},
  year      = {2013},
  publisher = {Curran Associates, Inc.},
  url       = {http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf}
}

@article{ba2016layer,
  title   = {Layer normalization},
  author  = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal = {arXiv preprint arXiv:1607.06450},
  year    = {2016}
}

@article{DBLP:journals/corr/IoffeS15,
  author        = {Sergey Ioffe and
                   Christian Szegedy},
  title         = {Batch Normalization: Accelerating Deep Network Training by Reducing
                   Internal Covariate Shift},
  journal       = {CoRR},
  volume        = {abs/1502.03167},
  year          = {2015},
  url           = {http://arxiv.org/abs/1502.03167},
  archiveprefix = {arXiv},
  eprint        = {1502.03167},
  timestamp     = {Mon, 13 Aug 2018 16:47:06 +0200},
  biburl        = {https://dblp.org/rec/bib/journals/corr/IoffeS15},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@article{manning2008irbook,
  title     = {Inroduction to Information Retrieval},
  author    = {Christopher D. Manning, Prabhakar Raghavan and Hinrich Schütze},
  publisher = {Cambridge University Press. 2008},
  year      = {2008},
  url       = {https://nlp.stanford.edu/IR-book/},
  publisher = {Now Publishers, Inc.}
}

@inproceedings{pennington2014glove,
  author    = {Jeffrey Pennington and Richard Socher and Christopher D. Manning},
  booktitle = {Empirical Methods in Natural Language Processing (EMNLP)},
  title     = {GloVe: Global Vectors for Word Representation},
  year      = {2014},
  pages     = {1532--1543},
  url       = {http://www.aclweb.org/anthology/D14-1162}
}

@article{hinton2012neural,
  title   = {Neural networks for machine learning lecture 6a overview of mini-batch gradient descent},
  journal = {University of Toronto, Technical Report},
  author  = {Hinton, Geoffrey and Srivastava, Nitish and Swersky, Kevin},
  year    = {2012}
}

@article{duchi2011adaptive,
  title   = {Adaptive subgradient methods for online learning and stochastic optimization},
  author  = {Duchi, John and Hazan, Elad and Singer, Yoram},
  journal = {Journal of Machine Learning Research},
  volume  = {12},
  number  = {Jul},
  pages   = {2121--2159},
  year    = {2011}
}

@article{kingma2014adam,
  title   = {Adam: A method for stochastic optimization},
  author  = {Kingma, Diederik P and Ba, Jimmy},
  journal = {arXiv preprint arXiv:1412.6980},
  year    = {2014}
}

@inproceedings{Peters:2018,
  author    = {Peters, Matthew E. and  Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  title     = {Deep contextualized word representations},
  booktitle = {Proc. of NAACL},
  year      = {2018}
}

@article{DBLP:journals/corr/KumarISBEPOGS15,
  author        = {Ankit Kumar and
                   Ozan Irsoy and
                   Jonathan Su and
                   James Bradbury and
                   Robert English and
                   Brian Pierce and
                   Peter Ondruska and
                   Ishaan Gulrajani and
                   Richard Socher},
  title         = {Ask Me Anything: Dynamic Memory Networks for Natural Language Processing},
  journal       = {CoRR},
  volume        = {abs/1506.07285},
  year          = {2015},
  url           = {http://arxiv.org/abs/1506.07285},
  archiveprefix = {arXiv},
  eprint        = {1506.07285},
  timestamp     = {Thu, 21 Mar 2019 11:19:44 +0100},
  biburl        = {https://dblp.org/rec/bib/journals/corr/KumarISBEPOGS15},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/WuSCLNMKCGMKSJL16,
  author        = {Yonghui Wu and
                   Mike Schuster and
                   Zhifeng Chen and
                   Quoc V. Le and
                   Mohammad Norouzi and
                   Wolfgang Macherey and
                   Maxim Krikun and
                   Yuan Cao and
                   Qin Gao and
                   Klaus Macherey and
                   Jeff Klingner and
                   Apurva Shah and
                   Melvin Johnson and
                   Xiaobing Liu and
                   Lukasz Kaiser and
                   Stephan Gouws and
                   Yoshikiyo Kato and
                   Taku Kudo and
                   Hideto Kazawa and
                   Keith Stevens and
                   George Kurian and
                   Nishant Patil and
                   Wei Wang and
                   Cliff Young and
                   Jason Smith and
                   Jason Riesa and
                   Alex Rudnick and
                   Oriol Vinyals and
                   Greg Corrado and
                   Macduff Hughes and
                   Jeffrey Dean},
  title         = {Google's Neural Machine Translation System: Bridging the Gap between
                   Human and Machine Translation},
  journal       = {CoRR},
  volume        = {abs/1609.08144},
  year          = {2016},
  url           = {http://arxiv.org/abs/1609.08144},
  archiveprefix = {arXiv},
  eprint        = {1609.08144},
  timestamp     = {Thu, 14 Mar 2019 09:34:18 +0100},
  biburl        = {https://dblp.org/rec/bib/journals/corr/WuSCLNMKCGMKSJL16},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@misc{tensorflow2015-whitepaper,
  title  = { {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
  url    = {https://www.tensorflow.org/},
  note   = {Software available from tensorflow.org},
  author = {
            Martìn~Abadi and
            Ashish~Agarwal and
            Paul~Barham and
            Eugene~Brevdo and
            Zhifeng~Chen and
            Craig~Citro and
            Greg~S.~Corrado and
            Andy~Davis and
            Jeffrey~Dean and
            Matthieu~Devin and
            Sanjay~Ghemawat and
            Ian~Goodfellow and
            Andrew~Harp and
            Geoffrey~Irving and
            Michael~Isard and
            Yangqing Jia and
            Rafal~Jozefowicz and
            Lukasz~Kaiser and
            Manjunath~Kudlur and
            Josh~Levenberg and
            Dandelion~Manè and
            Rajat~Monga and
            Sherry~Moore and
            Derek~Murray and
            Chris~Olah and
            Mike~Schuster and
            Jonathon~Shlens and
            Benoit~Steiner and
            Ilya~Sutskever and
            Kunal~Talwar and
            Paul~Tucker and
            Vincent~Vanhoucke and
            Vijay~Vasudevan and
            Fernanda~Viègas and
            Oriol~Vinyals and
            Pete~Warden and
            Martin~Wattenberg and
            Martin~Wicke and
            Yuan~Yu and
            Xiaoqiang~Zheng},
  year   = {2015}
}

@article{DBLP:journals/corr/abs-1901-04085,
  author        = {Rodrigo Nogueira and
                   Kyunghyun Cho},
  title         = {Passage Re-ranking with {BERT}},
  journal       = {CoRR},
  volume        = {abs/1901.04085},
  year          = {2019},
  url           = {http://arxiv.org/abs/1901.04085},
  archiveprefix = {arXiv},
  eprint        = {1901.04085},
  timestamp     = {Fri, 01 Feb 2019 13:39:59 +0100},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1901-04085},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1905-05910,
  author        = {Peng Xu and
                   Xiaofei Ma and
                   Ramesh Nallapati and
                   Bing Xiang},
  title         = {Passage Ranking with Weak Supervsion},
  journal       = {CoRR},
  volume        = {abs/1905.05910},
  year          = {2019},
  url           = {http://arxiv.org/abs/1905.05910},
  archiveprefix = {arXiv},
  eprint        = {1905.05910},
  timestamp     = {Tue, 28 May 2019 12:48:08 +0200},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1905-05910},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1804-07888,
  author        = {Xiaodong Liu and
                   Kevin Duh and
                   Jianfeng Gao},
  title         = {Stochastic Answer Networks for Natural Language Inference},
  journal       = {CoRR},
  volume        = {abs/1804.07888},
  year          = {2018},
  url           = {http://arxiv.org/abs/1804.07888},
  archiveprefix = {arXiv},
  eprint        = {1804.07888},
  timestamp     = {Mon, 13 Aug 2018 16:47:29 +0200},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1804-07888},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{burges2005learning,
  title     = {Learning to rank using gradient descent},
  author    = {Burges, Christopher and Shaked, Tal and Renshaw, Erin and Lazier, Ari and Deeds, Matt and Hamilton, Nicole and Hullender, Gregory N},
  booktitle = {Proceedings of the 22nd International Conference on Machine learning (ICML-05)},
  pages     = {89--96},
  year      = {2005}
}
@article{radford2018improving,
  title  = {Improving language understanding by generative pre-training},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year   = {2018}
}
@inproceedings{sukhbaatar2015end,
  title     = {End-to-end memory networks},
  author    = {Sukhbaatar, Sainbayar and Weston, Jason and Fergus, Rob and others},
  booktitle = {Advances in neural information processing systems},
  pages     = {2440--2448},
  year      = {2015}
}
@article{graves2014neural,
  title   = {Neural turing machines},
  author  = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
  journal = {arXiv preprint arXiv:1410.5401},
  year    = {2014}
}
@inproceedings{mitra2017learning,
  title        = {Learning to match using local and distributed representations of text for web search},
  author       = {Mitra, Bhaskar and Diaz, Fernando and Craswell, Nick},
  booktitle    = {Proceedings of the 26th International Conference on World Wide Web},
  pages        = {1291--1299},
  year         = {2017},
  organization = {International World Wide Web Conferences Steering Committee}
}
@inproceedings{dai2018convolutional,
  title        = {Convolutional neural networks for soft-matching n-grams in ad-hoc search},
  author       = {Dai, Zhuyun and Xiong, Chenyan and Callan, Jamie and Liu, Zhiyuan},
  booktitle    = {Proceedings of the eleventh ACM international conference on web search and data mining},
  pages        = {126--134},
  year         = {2018},
  organization = {ACM}
}
@book{Goodfellow-et-al-2016,
  title     = {Deep Learning},
  author    = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
  publisher = {MIT Press},
  note      = {\url{http://www.deeplearningbook.org}},
  year      = {2016}
}

@inproceedings{Tran:2018:NNF:3184558.3191830,
  author    = {Tran, Nam Khanh and Nieder{\'e}e, Claudia},
  title     = {A Neural Network-based Framework for Non-factoid Question Answering},
  booktitle = {Companion Proceedings of the The Web Conference 2018},
  series    = {WWW '18},
  year      = {2018},
  isbn      = {978-1-4503-5640-4},
  location  = {Lyon, France},
  pages     = {1979--1983},
  numpages  = {5},
  url       = {https://doi.org/10.1145/3184558.3191830},
  doi       = {10.1145/3184558.3191830},
  acmid     = {3191830},
  publisher = {International World Wide Web Conferences Steering Committee},
  address   = {Republic and Canton of Geneva, Switzerland},
  keywords  = {non-factoid question answering, representation learning}
} 

@inproceedings{zhu2015aligning,
  title     = {Aligning books and movies: Towards story-like visual explanations by watching movies and reading books},
  author    = {Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  booktitle = {Proceedings of the IEEE international conference on computer vision},
  pages     = {19--27},
  year      = {2015}
}

@article{mitra2018an,
  author   = {Mitra, Bhaskar and Craswell, Nick},
  title    = {An Introduction to Neural Information Retrieval},
  year     = {2018},
  abstract = {Neural ranking models for information retrieval (IR) use shallow or deep neural networks to rank search results in response to a query. Traditional learning to rank models employ supervised machine learning (ML) techniques—including neural networks—over hand-crafted IR features. By contrast, more recently proposed neural models learn representations of language from raw text that can bridge the gap between query and document vocabulary. Unlike classical learning to rank models and non-neural approaches to IR, these new ML techniques are data-hungry, requiring large scale training data before they can be deployed. This tutorial introduces basic concepts and intuitions behind neural IR models, and places them in the context of classical non-neural approaches to IR. We begin by introducing fundamental concepts of retrieval and different neural and non-neural approaches to unsupervised learning of vector representations of text. We then review IR methods that employ these pre-trained neural vector representations without learning the IR task end-to-end. We introduce the Learning to Rank (LTR) framework next, discussing standard loss functions for ranking. We follow that with an overview of deep neural networks (DNNs), including standard architectures and implementations. Finally, we review supervised neural learning to rank models, including recent DNN architectures trained end-to-end for ranking tasks. We conclude with a discussion on potential future directions for neural IR.},
  url      = {https://www.microsoft.com/en-us/research/publication/introduction-neural-information-retrieval/},
  pages    = {1-126},
  journal  = {Foundations and Trends® in Information Retrieval},
  volume   = {13},
  number   = {1}
}

@incollection{NEURIPS2019_9015,
  title     = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  author    = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  booktitle = {Advances in Neural Information Processing Systems 32},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages     = {8024--8035},
  year      = {2019},
  publisher = {Curran Associates, Inc.},
  url       = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@article{DBLP:journals/corr/abs-1207-0580,
  author     = {Geoffrey E. Hinton and
                Nitish Srivastava and
                Alex Krizhevsky and
                Ilya Sutskever and
                Ruslan Salakhutdinov},
  title      = {Improving neural networks by preventing co-adaptation of feature detectors},
  journal    = {CoRR},
  volume     = {abs/1207.0580},
  year       = {2012},
  url        = {http://arxiv.org/abs/1207.0580},
  eprinttype = {arXiv},
  eprint     = {1207.0580},
  timestamp  = {Mon, 13 Aug 2018 16:46:10 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1207-0580.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2010-11929,
  author     = {Alexey Dosovitskiy and
                Lucas Beyer and
                Alexander Kolesnikov and
                Dirk Weissenborn and
                Xiaohua Zhai and
                Thomas Unterthiner and
                Mostafa Dehghani and
                Matthias Minderer and
                Georg Heigold and
                Sylvain Gelly and
                Jakob Uszkoreit and
                Neil Houlsby},
  title      = {An Image is Worth 16x16 Words: Transformers for Image Recognition
                at Scale},
  journal    = {CoRR},
  volume     = {abs/2010.11929},
  year       = {2020},
  url        = {https://arxiv.org/abs/2010.11929},
  eprinttype = {arXiv},
  eprint     = {2010.11929},
  timestamp  = {Fri, 20 Nov 2020 14:04:05 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2010-11929.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@misc{https://doi.org/10.48550/arxiv.2005.00341,
  doi       = {10.48550/ARXIV.2005.00341},
  url       = {https://arxiv.org/abs/2005.00341},
  author    = {Dhariwal, Prafulla and Jun, Heewoo and Payne, Christine and Kim, Jong Wook and Radford, Alec and Sutskever, Ilya},
  keywords  = {Audio and Speech Processing (eess.AS), Machine Learning (cs.LG), Sound (cs.SD), Machine Learning (stat.ML), FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Jukebox: A Generative Model for Music},
  publisher = {arXiv},
  year      = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{DBLP:journals/corr/abs-1907-11692,
  author     = {Yinhan Liu and
                Myle Ott and
                Naman Goyal and
                Jingfei Du and
                Mandar Joshi and
                Danqi Chen and
                Omer Levy and
                Mike Lewis and
                Luke Zettlemoyer and
                Veselin Stoyanov},
  title      = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
  journal    = {CoRR},
  volume     = {abs/1907.11692},
  year       = {2019},
  url        = {http://arxiv.org/abs/1907.11692},
  eprinttype = {arXiv},
  eprint     = {1907.11692},
  timestamp  = {Thu, 01 Aug 2019 08:59:33 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2003-10555,
  author     = {Kevin Clark and
                Minh{-}Thang Luong and
                Quoc V. Le and
                Christopher D. Manning},
  title      = {{ELECTRA:} Pre-training Text Encoders as Discriminators Rather Than
                Generators},
  journal    = {CoRR},
  volume     = {abs/2003.10555},
  year       = {2020},
  url        = {https://arxiv.org/abs/2003.10555},
  eprinttype = {arXiv},
  eprint     = {2003.10555},
  timestamp  = {Wed, 01 Apr 2020 17:39:11 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2003-10555.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1909-08053,
  author     = {Mohammad Shoeybi and
                Mostofa Patwary and
                Raul Puri and
                Patrick LeGresley and
                Jared Casper and
                Bryan Catanzaro},
  title      = {Megatron-LM: Training Multi-Billion Parameter Language Models Using
                Model Parallelism},
  journal    = {CoRR},
  volume     = {abs/1909.08053},
  year       = {2019},
  url        = {http://arxiv.org/abs/1909.08053},
  eprinttype = {arXiv},
  eprint     = {1909.08053},
  timestamp  = {Tue, 24 Sep 2019 11:33:51 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1909-08053.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2005-14165,
  author     = {Tom B. Brown and
                Benjamin Mann and
                Nick Ryder and
                Melanie Subbiah and
                Jared Kaplan and
                Prafulla Dhariwal and
                Arvind Neelakantan and
                Pranav Shyam and
                Girish Sastry and
                Amanda Askell and
                Sandhini Agarwal and
                Ariel Herbert{-}Voss and
                Gretchen Krueger and
                Tom Henighan and
                Rewon Child and
                Aditya Ramesh and
                Daniel M. Ziegler and
                Jeffrey Wu and
                Clemens Winter and
                Christopher Hesse and
                Mark Chen and
                Eric Sigler and
                Mateusz Litwin and
                Scott Gray and
                Benjamin Chess and
                Jack Clark and
                Christopher Berner and
                Sam McCandlish and
                Alec Radford and
                Ilya Sutskever and
                Dario Amodei},
  title      = {Language Models are Few-Shot Learners},
  journal    = {CoRR},
  volume     = {abs/2005.14165},
  year       = {2020},
  url        = {https://arxiv.org/abs/2005.14165},
  eprinttype = {arXiv},
  eprint     = {2005.14165},
  timestamp  = {Wed, 03 Jun 2020 11:36:54 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2005-14165.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{xu2015show,
  title        = {Show, attend and tell: Neural image caption generation with visual attention},
  author       = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhudinov, Ruslan and Zemel, Rich and Bengio, Yoshua},
  booktitle    = {International conference on machine learning},
  pages        = {2048--2057},
  year         = {2015},
  organization = {PMLR}
}

@article{xiong2016dynamic,
  title   = {Dynamic coattention networks for question answering},
  author  = {Xiong, Caiming and Zhong, Victor and Socher, Richard},
  journal = {arXiv preprint arXiv:1611.01604},
  year    = {2016}
}


@article{DBLP:journals/corr/abs-1801-06146,
  author     = {Jeremy Howard and
                Sebastian Ruder},
  title      = {Fine-tuned Language Models for Text Classification},
  journal    = {CoRR},
  volume     = {abs/1801.06146},
  year       = {2018},
  url        = {http://arxiv.org/abs/1801.06146},
  eprinttype = {arXiv},
  eprint     = {1801.06146},
  timestamp  = {Mon, 13 Aug 2018 16:46:54 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1801-06146.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@inbook{10.5555/3454287.3454804,
  author    = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V.},
  title     = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},
  year      = {2019},
  publisher = {Curran Associates Inc.},
  address   = {Red Hook, NY, USA},
  abstract  = {With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment setting, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.},
  booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
  articleno = {517},
  numpages  = {11}
}

@inproceedings{Lan2020ALBERT,
  title     = {ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
  author    = {Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
  booktitle = {International Conference on Learning Representations},
  year      = {2020},
  url       = {https://openreview.net/forum?id=H1eA7AEtvS}
}

@article{Bender2021OnTD,
  title   = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
  author  = {Emily M. Bender and Timnit Gebru and Angelina McMillan-Major and Shmargaret Shmitchell},
  journal = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
  year    = {2021}
}

@inproceedings{Nadeem2021StereoSetMS,
  title     = {StereoSet: Measuring stereotypical bias in pretrained language models},
  author    = {Moin Nadeem and Anna Bethke and Siva Reddy},
  booktitle = {ACL},
  year      = {2021}
}

@article{kurita2019measuring,
  title   = {Measuring bias in contextualized word representations},
  author  = {Kurita, Keita and Vyas, Nidhi and Pareek, Ayush and Black, Alan W and Tsvetkov, Yulia},
  journal = {arXiv preprint arXiv:1906.07337},
  year    = {2019}
}

@inproceedings{10.1145/2939672.2939778,
  author    = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  title     = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
  year      = {2016},
  isbn      = {9781450342322},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2939672.2939778},
  doi       = {10.1145/2939672.2939778},
  abstract  = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
  booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages     = {1135–1144},
  numpages  = {10},
  keywords  = {explaining machine learning, interpretability, interpretable machine learning, black box classifier},
  location  = {San Francisco, California, USA},
  series    = {KDD '16}
}

@article{DBLP:journals/corr/abs-1802-00614,
  author     = {Quanshi Zhang and
                Song{-}Chun Zhu},
  title      = {Visual Interpretability for Deep Learning: a Survey},
  journal    = {CoRR},
  volume     = {abs/1802.00614},
  year       = {2018},
  url        = {http://arxiv.org/abs/1802.00614},
  eprinttype = {arXiv},
  eprint     = {1802.00614},
  timestamp  = {Mon, 13 Aug 2018 16:46:30 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1802-00614.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{7410368,
  author    = {Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
  title     = {Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books},
  year      = {2015},
  volume    = {},
  number    = {},
  pages     = {19-27},
  doi       = {10.1109/ICCV.2015.11}
}

@article{DBLP:journals/corr/abs-2003-07820,
  author     = {Nick Craswell and
                Bhaskar Mitra and
                Emine Yilmaz and
                Daniel Campos and
                Ellen M. Voorhees},
  title      = {Overview of the {TREC} 2019 deep learning track},
  journal    = {CoRR},
  volume     = {abs/2003.07820},
  year       = {2020},
  url        = {https://arxiv.org/abs/2003.07820},
  eprinttype = {arXiv},
  eprint     = {2003.07820},
  timestamp  = {Wed, 27 Apr 2022 14:24:37 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2003-07820.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{pimentel-etal-2020-information,
  title     = {Information-Theoretic Probing for Linguistic Structure},
  author    = {Pimentel, Tiago  and
               Valvoda, Josef  and
               Hall Maudslay, Rowan  and
               Zmigrod, Ran  and
               Williams, Adina  and
               Cotterell, Ryan},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  month     = jul,
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2020.acl-main.420},
  doi       = {10.18653/v1/2020.acl-main.420},
  pages     = {4609--4622},
  abstract  = {The success of neural networks on a diverse set of NLP tasks has led researchers to question how much these networks actually {``}know{''} about natural language. Probes are a natural way of assessing this. When probing, a researcher chooses a linguistic task and trains a supervised model to predict annotations in that linguistic task from the network{'}s learned representations. If the probe does well, the researcher may conclude that the representations encode knowledge related to the task. A commonly held belief is that using simpler models as probes is better; the logic is that simpler models will identify linguistic structure, but not learn the task itself. We propose an information-theoretic operationalization of probing as estimating mutual information that contradicts this received wisdom: one should always select the highest performing probe one can, even if it is more complex, since it will result in a tighter estimate, and thus reveal more of the linguistic information inherent in the representation. The experimental portion of our paper focuses on empirically estimating the mutual information between a linguistic property and BERT, comparing these estimates to several baselines. We evaluate on a set of ten typologically diverse languages often underrepresented in NLP research{---}plus English{---}totalling eleven languages. Our implementation is available in https://github.com/rycolab/info-theoretic-probing.}
}

@article{Tenney2019WhatDY,
  title   = {What do you learn from context? Probing for sentence structure in contextualized word representations},
  author  = {Ian Tenney and Patrick Xia and Berlin Chen and Alex Wang and Adam Poliak and R. Thomas McCoy and Najoung Kim and Benjamin Van Durme and Samuel R. Bowman and Dipanjan Das and Ellie Pavlick},
  journal = {ArXiv},
  year    = {2019},
  volume  = {abs/1905.06316}
}

@inproceedings{tenney-etal-2019-bert,
  title     = {{BERT} Rediscovers the Classical {NLP} Pipeline},
  author    = {Tenney, Ian  and
               Das, Dipanjan  and
               Pavlick, Ellie},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  month     = jul,
  year      = {2019},
  address   = {Florence, Italy},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/P19-1452},
  doi       = {10.18653/v1/P19-1452},
  pages     = {4593--4601},
  abstract  = {Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations.}
}

@inproceedings{hewitt-liang-2019-designing,
  title     = {Designing and Interpreting Probes with Control Tasks},
  author    = {Hewitt, John  and
               Liang, Percy},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  month     = nov,
  year      = {2019},
  address   = {Hong Kong, China},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/D19-1275},
  doi       = {10.18653/v1/D19-1275},
  pages     = {2733--2743},
  abstract  = {Probes, supervised models trained to predict properties (like parts-of-speech) from representations (like ELMo), have achieved high accuracy on a range of linguistic tasks. But does this mean that the representations encode linguistic structure or just that the probe has learned the linguistic task? In this paper, we propose control tasks, which associate word types with random outputs, to complement linguistic tasks. By construction, these tasks can only be learned by the probe itself. So a good probe, (one that reflects the representation), should be selective, achieving high linguistic task accuracy and low control task accuracy. The selectivity of a probe puts linguistic task accuracy in context with the probe{'}s capacity to memorize from word types. We construct control tasks for English part-of-speech tagging and dependency edge prediction, and show that popular probes on ELMo representations are not selective. We also find that dropout, commonly used to control probe complexity, is ineffective for improving selectivity of MLPs, but that other forms of regularization are effective. Finally, we find that while probes on the first layer of ELMo yield slightly better part-of-speech tagging accuracy than the second, probes on the second layer are substantially more selective, which raises the question of which layer better represents parts-of-speech.}
}

@inproceedings{lee-etal-2017-end,
  title     = {End-to-end Neural Coreference Resolution},
  author    = {Lee, Kenton  and
               He, Luheng  and
               Lewis, Mike  and
               Zettlemoyer, Luke},
  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  month     = sep,
  year      = {2017},
  address   = {Copenhagen, Denmark},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/D17-1018},
  doi       = {10.18653/v1/D17-1018},
  pages     = {188--197},
  abstract  = {We introduce the first end-to-end coreference resolution model and show that it significantly outperforms all previous work without using a syntactic parser or hand-engineered mention detector. The key idea is to directly consider all spans in a document as potential mentions and learn distributions over possible antecedents for each. The model computes span embeddings that combine context-dependent boundary representations with a head-finding attention mechanism. It is trained to maximize the marginal likelihood of gold antecedent spans from coreference clusters and is factored to enable aggressive pruning of potential mentions. Experiments demonstrate state-of-the-art performance, with a gain of 1.5 F1 on the OntoNotes benchmark and by 3.1 F1 using a 5-model ensemble, despite the fact that this is the first approach to be successfully trained with no external resources.}
}

@inproceedings{lee-etal-2018-higher,
  title     = {Higher-Order Coreference Resolution with Coarse-to-Fine Inference},
  author    = {Lee, Kenton  and
               He, Luheng  and
               Zettlemoyer, Luke},
  booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)},
  month     = jun,
  year      = {2018},
  address   = {New Orleans, Louisiana},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/N18-2108},
  doi       = {10.18653/v1/N18-2108},
  pages     = {687--692},
  abstract  = {We introduce a fully-differentiable approximation to higher-order inference for coreference resolution. Our approach uses the antecedent distribution from a span-ranking architecture as an attention mechanism to iteratively refine span representations. This enables the model to softly consider multiple hops in the predicted clusters. To alleviate the computational cost of this iterative process, we introduce a coarse-to-fine approach that incorporates a less accurate but more efficient bilinear factor, enabling more aggressive pruning without hurting accuracy. Compared to the existing state-of-the-art span-ranking approach, our model significantly improves accuracy on the English OntoNotes benchmark, while being far more computationally efficient.}
}

@inproceedings{thorne-etal-2018-fever,
  title     = {{FEVER}: a Large-scale Dataset for Fact Extraction and {VER}ification},
  author    = {Thorne, James  and
               Vlachos, Andreas  and
               Christodoulopoulos, Christos  and
               Mittal, Arpit},
  booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
  month     = jun,
  year      = {2018},
  address   = {New Orleans, Louisiana},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/N18-1074},
  doi       = {10.18653/v1/N18-1074},
  pages     = {809--819},
  abstract  = {In this paper we introduce a new publicly available dataset for verification against textual sources, FEVER: Fact Extraction and VERification. It consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. The claims are classified as Supported, Refuted or NotEnoughInfo by annotators achieving 0.6841 in Fleiss kappa. For the first two classes, the annotators also recorded the sentence(s) forming the necessary evidence for their judgment. To characterize the challenge of the dataset presented, we develop a pipeline approach and compare it to suitably designed oracles. The best accuracy we achieve on labeling a claim accompanied by the correct evidence is 31.87{\%}, while if we ignore the evidence we achieve 50.91{\%}. Thus we believe that FEVER is a challenging testbed that will help stimulate progress on claim verification against textual sources.}
}

@inproceedings{zhang-bowman-2018-language,
  title     = {Language Modeling Teaches You More than Translation Does: Lessons Learned Through Auxiliary Syntactic Task Analysis},
  author    = {Zhang, Kelly  and
               Bowman, Samuel},
  booktitle = {Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}},
  month     = nov,
  year      = {2018},
  address   = {Brussels, Belgium},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/W18-5448},
  doi       = {10.18653/v1/W18-5448},
  pages     = {359--361},
  abstract  = {Recently, researchers have found that deep LSTMs trained on tasks like machine translation learn substantial syntactic and semantic information about their input sentences, including part-of-speech. These findings begin to shed light on why pretrained representations, like ELMo and CoVe, are so beneficial for neural language understanding models. We still, though, do not yet have a clear understanding of how the choice of pretraining objective affects the type of linguistic information that models learn. With this in mind, we compare four objectives{---}language modeling, translation, skip-thought, and autoencoding{---}on their ability to induce syntactic and part-of-speech information, holding constant the quantity and genre of the training data, as well as the LSTM architecture.}
}

@article{DBLP:journals/corr/abs-1909-03368,
  author     = {John Hewitt and
                Percy Liang},
  title      = {Designing and Interpreting Probes with Control Tasks},
  journal    = {CoRR},
  volume     = {abs/1909.03368},
  year       = {2019},
  url        = {http://arxiv.org/abs/1909.03368},
  eprinttype = {arXiv},
  eprint     = {1909.03368},
  timestamp  = {Tue, 17 Sep 2019 11:23:44 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1909-03368.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{voita-titov-2020-information,
  title     = {Information-Theoretic Probing with Minimum Description Length},
  author    = {Voita, Elena  and
               Titov, Ivan},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  month     = nov,
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2020.emnlp-main.14},
  doi       = {10.18653/v1/2020.emnlp-main.14},
  pages     = {183--196},
  abstract  = {To measure how well pretrained representations encode some linguistic property, it is common to use accuracy of a probe, i.e. a classifier trained to predict the property from the representations. Despite widespread adoption of probes, differences in their accuracy fail to adequately reflect differences in representations. For example, they do not substantially favour pretrained representations over randomly initialized ones. Analogously, their accuracy can be similar when probing for genuine linguistic labels and probing for random synthetic tasks. To see reasonable differences in accuracy with respect to these random baselines, previous work had to constrain either the amount of probe training data or its model size. Instead, we propose an alternative to the standard probes, information-theoretic probing with minimum description length (MDL). With MDL probing, training a probe to predict labels is recast as teaching it to effectively transmit the data. Therefore, the measure of interest changes from probe accuracy to the description length of labels given representations. In addition to probe quality, the description length evaluates {``}the amount of effort{''} needed to achieve the quality. This amount of effort characterizes either (i) size of a probing model, or (ii) the amount of data needed to achieve the high quality. We consider two methods for estimating MDL which can be easily implemented on top of the standard probing pipelines: variational coding and online coding. We show that these methods agree in results and are more informative and stable than the standard probes.}
}

@article{Rissanen1984UniversalCI,
  title={Universal coding, information, prediction, and estimation},
  author={Jorma Rissanen},
  journal={IEEE Trans. Inf. Theory},
  year={1984},
  volume={30},
  pages={629-636}
}