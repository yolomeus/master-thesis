\chapter{Previous Work}
\label{chap:prev}
As this is a two part thesis, there are mainly two fields of work it builds upon. In this section we will first introduce related work in neural information retrieval that leverage pre-trained transformer models. We then provide a quick overview of previous work in probing for NLP, as well as Multi-task learning approaches.

\section{Neural IR with Transformers}
In this section we present work on text ranking using transformers. If the reader is interested in a general overview of neural information retrieval, we recommend \cite{mitra2018an, Onal2017NeuralIR, Guo2020ADL}.

With the breakthrough of BERT \cite{devlin-etal-2019-bert} achieving state-of-the-art in various NLP benchmarks, it didn't take long for researchers to also try applying the pre-trained BERT model to the information retrieval problem. In \cite{Nogueira2019PassageRW}, a BERT model is fine-tuned on the MS Marco dataset \cite{DBLP:journals/corr/NguyenRSGTMD16} to perform re-ranking of text passages. Even though a simple approach was used, which treated relevance estimation as binary classification, it outperformed the existing baselines on the MS Marco leaderboard at that time. Now often termed monoBERT, this approach has become a popular baseline for neural text ranking.

Later, in \cite{Nogueira2019MultiStageDR} a new ranking objective is proposed that computes relevance scores over all possible document pairs in a candidate set. This pairwise objective is titled duoBERT and achieves better ranking performance with a quadratic increase in computational cost w.r.t. the size of the candidate set. \cite{DBLP:journals/corr/abs-1912-01385}, on the other hand, do not rely on pre-trained models and instead redesign a neural ranking architecture based on transformer encoders to be more efficient, titled transformer kernel. Further variants building upon this approach are introduced in \cite{Hofsttter2020LocalSO, 10.1145/3404835.3463049}.

Unlike the previously mentioned methods that leverage a single model to estimate relevance of query-document pairs, \ti{dense retrieval} approaches focus on pre-computing embeddings and directly operating on them during the retrieval phase, e.g. by nearest neighbor search. To achieve this, the common approach is to use two separate encoder models (e.g. two BERT models) for document and query, that produce a fixed size embedding for both. The encoders can then be trained such that a query and a relevant document result in similar embeddings, e.g. by optimizing for cosine similarity between both vectors \cite{Humeau2020Poly-encoders, 10.1145/3397271.3401075, DBLP:journals/corr/abs-1908-10084}.

Additional methods for transformer based ranking include knowledge distillation \cite{DBLP:journals/corr/abs-2009-07531, DBLP:journals/corr/abs-2010-02666}, cascading models for efficiency \cite{Nogueira2019MultiStageDR} and many more. For a more comprehensive survey on transformers in the context of IR, we refer the reader to \cite{10.1145/3437963.3441667}.

\section{Probing Language Models}
\cite{conneau-etal-2018-cram} employ $10$ linguistic probing tasks in order to evaluate the representations of different deep sentence encoders. They find that compared to simple baselines, these models are better at encoding complex linguistic properties.

On the other hand, \cite{Belinkov2017WhatDN} probe neural machine translation models for part-of-speech information and show evidence that morphological features are better captured at lower layers of these models. Likewise, contextualized embeddings of three different bidirectional language model architectures are probed in \cite{peters-etal-2018-dissecting}, with the findings suggesting that lower layers specialize in local syntactic relationships while higher layers model longer range semantic relationships. Supporting this, \cite{tenney-etal-2019-bert} deconstruct the language abilities of a BERT model and find that the distribution of linguistic properties across layers suggests hierarchical feature extraction, similar to classical NLP approaches.

In \cite{Tenney2019WhatDY} a general probing framework termed edge-probing is presented, which allows linguistic probing for numerous kinds of tasks. The framework is then used to analyze the impact of contextualization of embeddings. They show that contextualized representations are also generally better at encoding syntactic information when compared to their non-contextualized counterparts. Beyond that, \cite{Hewitt2019ASP} design a probe which is able to verify that BERT encodes full syntax trees within its embeddings.

\cite{merchant-etal-2020-happens} further investigate the effect of fine-tuning on BERT embeddings. They find that significant information is retained during fine-tuning and that mostly higher level layers are affected.

On the other hand, \cite{Fayyaz2021NotAM} probe different popular transformer models (including BERT) in a layer-wise manner for linguistic knowledge, reporting differences between model types.

In another study \cite{singh-etal-2020-bertnesia} specifically probe BERT for the distribution of knowledge across different layers. Further, they do not limit their experiments to the base model, but also investigate fine-tuned models, including ranking as downstream task. They Find that a significant amount of knowledge is stored at intermediate layers.

Highly related to this work, \cite{https://doi.org/10.48550/arxiv.2202.12191} probe BERT for the presence of IDF scores across layers. They find IDF to be mostly captured by early layers.

\section{Multitask Learning}
There have been several approaches to address NLP problems with multitask learning. In \cite{10.1145/1390156.1390177} a convolutional neural network architecture is proposed that jointly learns 6 different NLP tasks, including part-of-speech tagging, named entity recognition and language modeling.

More recently, \cite{DBLP:journals/corr/abs-1811-01088} introduce an additional training stage to BERT, in which BERT is trained on a supplementary task before fine-tuning. Similarly, \cite{DBLP:journals/corr/abs-1901-11504} also perform additional training prior to fine-tuning, however they train on multiple tasks simultaneously.

\cite{maillard-etal-2021-multi} present a method for multitask learning, to learn a neural ranking model that is better at generalizing across different domains. Likewise, \cite{Fun2021EfficientRO} present a retrieval-optimized multitask learning approach, but make use of a sequential training schedule.