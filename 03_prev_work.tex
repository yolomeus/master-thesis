\chapter{Related Work}
\label{chap:prev}
There are mainly three fields of work this thesis builds upon. In this section we will first introduce related work in neural information retrieval that leverage pre-trained transformer models. We then provide a quick overview of previous work in probing for NLP, as well as multi-task learning approaches.

\section{Neural IR with Transformers}
In this section we present work on text ranking using transformers. If the reader is interested in a general overview of neural information retrieval, we recommend \citep{Onal2017NeuralIR, mitra2018an, Guo2020ADL}.

With the breakthrough of BERT \citep{devlin-etal-2019-bert} achieving state-of-the-art in various NLP benchmarks, it didn't take long for researchers to also try applying the pre-trained BERT model to the information retrieval problem. In \citep{Nogueira2019PassageRW}, a BERT model is fine-tuned on the MS Marco dataset \citep{DBLP:journals/corr/NguyenRSGTMD16} to perform re-ranking of text passages. Even though a simple approach was used, which treated relevance estimation as binary classification, it outperformed the existing baselines on the MS Marco leaderboard at that time. Now often termed monoBERT, this approach has become a popular baseline for neural text ranking.

Later, in \citep{Nogueira2019MultiStageDR} a new ranking objective is proposed that computes relevance scores over all possible document pairs in a candidate set. This pair-wise objective is titled duoBERT and achieves better ranking performance with a quadratic increase in computational cost with respect to the size of the candidate set. \citet{DBLP:journals/corr/abs-1912-01385}, on the other hand, do not rely on pre-trained models and instead redesign a neural ranking architecture based on transformer encoders to be more efficient, titled transformer kernel. Further variants building upon this approach are introduced in \citep{Hofsttter2020LocalSO, 10.1145/3404835.3463049}.

Unlike the previously mentioned methods that leverage a single model to estimate relevance of query-document pairs, often referred to as \ti{cross encoders}, \ti{dense retrieval} approaches focus on pre-computing embeddings and directly operating on them during the retrieval phase, e.g. by nearest neighbor search. To achieve this, the common approach is to use two separate encoder models (e.g. two BERT models) for document and query, that produce a fixed size embedding for both. The encoders can then be trained such that a query and a relevant document result in similar embeddings, e.g. by optimizing for cosine similarity between both vectors \citep{Humeau2020Poly-encoders, 10.1145/3397271.3401075, DBLP:journals/corr/abs-1908-10084}.

While dense-retrieval methods are generally faster at retrieval time, they usually sacrifice ranking performance when compared to cross encoders.

Further methods for transformer based ranking include knowledge distillation \citep{DBLP:journals/corr/abs-2009-07531, DBLP:journals/corr/abs-2010-02666}, cascading models for efficiency \citep{Nogueira2019MultiStageDR} and many more. For a more comprehensive survey on transformers in the context of IR, we refer the reader to \citep{10.1145/3437963.3441667}.

\section{Probing Language Models}
Recently, a body of research has emerged that focuses on understanding dense vector representations of pre-trained neural language models. Through \ti{probing}, it can be inferred how well certain linguistic properties can be decoded from such representations by predicting these properties with a diagnostic classifier. For instance, \citet{conneau-etal-2018-cram} employ $10$ linguistic probing tasks in order to evaluate the representations of different deep sentence encoders. Not only do they probe for simple sentence statistics such as sentence length and word-count, but also for more complex syntactic properties like syntax tree depth. They find that compared to bag of words baselines, these models are generally better at encoding linguistic properties. Similarly, \citet{Hewitt2019ASP} design a probe which is able to verify that even full syntax trees can be decoded from a BERT model.

Different from \citet{conneau-etal-2018-cram}, \citet{Belinkov2017WhatDN} probe neural machine translation models at \ti{multiple layers}. By employing part-of-speech prediction as probing task, they show evidence that morphological features are better captured at lower layers of these models. Likewise, contextualized embeddings of three different bidirectional language model architectures are probed in \citep{peters-etal-2018-dissecting}, with the findings suggesting that lower layers specialize in local syntactic relationships while higher layers model longer range semantic relationships. Supporting this further, \citet{tenney-etal-2019-bert} deconstruct the language abilities of a BERT model by probing for tasks on different semantic levels. They find that the distribution of linguistic properties across layers suggests hierarchical feature extraction, similar to classical NLP pipelines.

In \citep{Tenney2019WhatDY}, a general probing framework termed \ti{edge-probing} is presented, which allows linguistic probing for numerous kinds of tasks. The framework is then used to analyze the impact of contextualization of embeddings. They show that contextualized representations are also generally better at encoding syntactic information when compared to their non-contextualized counterparts.

In another study, \citet{singh-etal-2020-bertnesia} specifically probe BERT for the distribution of knowledge across different layers. In addition, they do not limit their experiments to the base model, but also investigate fine-tuned models, including ranking as a downstream task. They find that a significant amount of knowledge is stored at intermediate layers and that ranking models suffer less from forgetting than those trained for span prediction tasks. \citet{merchant-etal-2020-happens}, further investigate the effects of fine-tuning on BERT embeddings. Coinciding with \citet{singh-etal-2020-bertnesia}, they find that most information is retained during fine-tuning and that primarily higher level layers are affected by forgetting.

Closely related to this thesis, \citet{https://doi.org/10.48550/arxiv.2202.12191} probe BERT in the context of ranking. They evaluate whether IDF scores can be decoded from BERT embeddings at different layers of the model and find IDF information to be mostly captured at early layers.

While the general framework of applying a probe classifier to hidden vector representations of a neural language model has been widely adopted, it also comes with drawbacks. On its own, a probe's accuracy on a given task does not convey how well the probed property is encoded, as it is not clear how hard it is to achieve said accuracy. For instance, there is no indication on whether achieving $80\%$ accuracy on a particular task is good or not.
One approach to tackle this issue is to introduce a random baseline, i.e. a model with randomly initialized parameters. By probing both, the random baseline and the subject model at hand, a comparison of probe accuracy between both models can be made. Unfortunately, it is a common observation that the difference in accuracy is rather low which makes it hard to draw conclusions from such a comparison \citep{zhang-bowman-2018-language}.
Furthermore, it is not clear whether a probe simply learns the task, i.e. memorizes a mapping from embeddings to labels, or actually decodes the property from the word representations. To tackle this issue \citet{hewitt-liang-2019-designing} propose the idea of \ti{control tasks}. They construct random tasks by label perturbation on the original tasks. The difference in probe accuracy between both tasks is then termed \ti{selectivity} and used to quantify a probe's ability in decoding task information from the model's embeddings. Unfortunately, analogous to random baselines, this approach often suffers from differences in accuracy that are too small which \citet{hewitt-liang-2019-designing} address by limiting dataset size. Moreover, these control tasks require manual design, introducing additional overhead for each new task.

Due to the constraints of random baselines and control tasks, \citep{voita-titov-2020-information} propose a new metric to quantify probe performance which is the \ti{minimum description length} (MDL) of a probe. The metric is grounded on information theory \citep{shannon1948mathematical} and designed to not only reflect the performance of a probe, but also the effort required for achieving that performance. It neither requires the manual design of control tasks nor additional probing on a random baseline. \citep{voita-titov-2020-information} empirically show the metric to be more robust than accuracy. For more details on MDL, please refer to \Cref{sec:mdl}.

\section{Multi-task Learning for NLP}
Multi-task learning (MTL) encompasses methods that learn a single model from multiple tasks or datasets. One motivation for this is to learn a model that is better at generalizing across different tasks. In this setting, each task is treated equally important.

Another motivation is to improve a model's performance on a particular task by providing additional information from auxiliary tasks. In this case, performance on the main task is most important.

Due to the increased complexity of having to account for multiple tasks, MTL comes with a variety of possible design choices, regarding both architecture and training setup. For instance, \citet{DBLP:journals/corr/abs-2109-09138} name three general MTL architecture categories:
\begin{itemize}
    \item Tree-like architectures: a single encoder model is followed by task-specific decoders.
    \item Parallel feature fusion: each task has a single model, however these models fuse features on one or multiple levels.
    \item Supervision at different levels: task specific decoders are applied at different levels of a single model.
\end{itemize}
Another design choice is how to sample data from multiple datasets. For example, a straight forward approach is to sample proportionally to each dataset size \citep{DBLP:journals/corr/abs-1811-06031}. However, this can become problematic in cases where some datasets are simply too small compared to the other tasks. Another possibility is to perform task-oriented sampling \citep{DBLP:journals/corr/ZhangXWJ17} where samples for each task are drawn with the same probability. As these are just some of the design choices that need to be considered for MTL. For a more comprehensive overview we recommend \citep{DBLP:journals/corr/abs-2109-09138, DBLP:journals/corr/abs-2007-16008}.

There have been several approaches to address NLP problems with multi-task learning. For instance, \citep{10.1145/1390156.1390177} proposed a convolutional neural network architecture that jointly learns 6 different NLP tasks, including part-of-speech tagging, named entity recognition and language modeling.

More recently, in the context of transformer models, \citet{DBLP:journals/corr/abs-1811-01088} introduce an additional training stage to BERT, in which BERT is trained on a supplementary task before fine-tuning. Similarly, \citet{DBLP:journals/corr/abs-1901-11504} also perform additional training prior to fine-tuning, however using a tree-like architecture, they train on multiple tasks simultaneously.

In \citep{DBLP:journals/corr/abs-1910-10683}, a framework is proposed that enables casting of multiple language tasks into a single text-to-text format, allowing training those tasks with the same objective. They were not able to find sampling proportions that are capable of producing results comparable to a pre-train then fine-tune approach. However, when applying MTL as an intermediate step they've observed improved performance similar to \citet{DBLP:journals/corr/abs-1901-11504}.

\citet{aghajanyan-etal-2021-muppet} expand on that idea and perform large scale pre-finetuning using over $50$ tasks. They propose a simple, yet effective loss scaling approach and find that the intermediate training stage is also more effective the more tasks are supplied. Furthermore, they found the use of mixed-batches, i.e. mini-batches that contain samples from multiple tasks to be the most effective which they sampled proportionally to dataset size.

In the context of neural ranking, \citet{maillard-etal-2021-multi} present a method for multi-task learning that improves generalization across different domains. Likewise, \citet{Fun2021EfficientRO} present a retrieval-optimized multi-task learning approach, but make use of a sequential training schedule.