\chapter{Approach}
\label{chap:approach}

\section{Methodology}
Our goal is to understand whether certain properties that we expect to be relevant for ranking can be decoded from the hidden representations of a pre-trained transformer model. Further, we want to know to which degree these properties are encoded at different layers within the model.

To test this, we conduct the following experiment: First, we generate a set of datasets $D_{\tx{probing}}=\{D_1, D_2, \dots, D_n \}$, each aimed at predicting a property $Y_i$ that, based on traditional ranking methods, can be considered relevant for ranking. Then, for each dataset, we train a probing classifier $P_i: \R^d \rightarrow \R^c$ on top of the fixed hidden representations $H\lay k \in \R^{N \times d}$ of subject model $\mathcal{M}$. This procedure is repeated at every layer $k$. Finally, we compare the classifier's performance across layers, to get a relative measure of how task-specific information is distributed throughout the model. To further put our measurements into perspective, we also employ a random baseline model $\mathcal{B}$ which is also probed for each task separately.

Following the probing experiments, we then attempt to develop an improved fine-tuning procedure, based on our findings.

\subsection{Models}
\subsubsection{Subject Models}
Subject of our probing experiments is the pre-trained BERT~\cite{devlin-etal-2019-bert} transformer model. In particular, we use the \ti{bert-base-uncased}\footnote{\url{https://huggingface.co/bert-base-uncased}} variant, which consists of $12$ layers, with $h=12$ attention heads each and a hidden dimension of $d=786$. The model has been trained on BooksCorpus~\cite{7410368} and text passages from English Wikipedia\footnote{\url{https://en.m.wikipedia.org/}}, which consist of 800 million and 2.5 billion words, respectively.

In addition, we probe two fine-tuned \ti{bert-base-uncased} models that we term \ti{bert-msm-passage} and \ti{bert-msm-doc}. Both are trained on datasets from the TREC2019 deep learning track~\cite{DBLP:journals/corr/abs-2003-07820}. While \ti{bert-msm-passage} is trained to predict relevancy of \ti{passages} given a query, \ti{bert-msm-doc} is trained on \ti{document}-level data. Hence, for this purpose we use the TREC2019 passage- and document-level dataset respectively.

Note that all three models share the same architecture and only differ by which datasets they were trained on. Having access to additional versions of \ti{bert-base-uncased} that were fine-tuned specifically for ranking, gives us a way to compare if and how the distribution of information throughout the model changes when being adapted to the new task.

\subsubsection{Probe}



\section{Experimental Setup}
\subsection{Fine-tuning Subject Models}
\subsection{Probing Dataset Generation}

\section{Evaluation Measures}
\subsection{MDL}
\subsection{Compression}
\subsection{F1}
\subsection{Accuracy}
\subsection{Ranking}
\subsubsection{MAP}
\subsubsection{MRR}
\subsubsection{NDCG}
\subsubsection{Precision}

