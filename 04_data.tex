\chapter{Datasets}
\label{chap:datasets}

\section{TREC 2019 - Deep Learning Track}
\label{sec:trec2019}
The TREC 2019 deep learning track focuses on studying text retrieval on large-scale data \cite{DBLP:journals/corr/abs-2003-07820}. It provides two datasets, one for passage retrieval and one for document retrieval. The datasets are based on MS MARCO \cite{DBLP:journals/corr/NguyenRSGTMD16} which consists of $\sim 1$mio real world user queries from the Bing search engine and a corpus of $\sim 8.8$mio passages. For this thesis we focus on the passage retrieval dataset (TREC2019) if not stated otherwise. Further, we will refer to passages from this dataset as documents, to be consistent with the common information retrieval terminology.

\begin{table}[h]
    \centering
    \begin{tabular}{c|ccc}
        \hline
        \tf{Dataset} & \tf{Train} & \tf{Validation} & \tf{Test} \\ \hline
        Passage      & 502,939    & 55,578          & 200       \\ \hline
        Document     & 367,013    & 5,193           & 200       \\ \hline
    \end{tabular}
    \caption{Number of queries for each dataset split in the two TREC 2019 datasets.}
\end{table}

\begin{table}[h]
    \centering

    \begin{tabular}{c|c}
        \hline
        Passage   & Document  \\ \hline
        8,841,823 & 3,213,835 \\ \hline
    \end{tabular}
    \caption{Corpus size for each TREC dataset.}
\end{table}

While with TREC2019, two types of tasks are provided, namely full ranking and re-ranking, we will only perform the re-ranking task. This means, given a pool of $1000$ documents for each query, we need to provide an ordering, such that relevant documents are placed at the top. On average, a query has $\sim 1.1$ relevant documents in its pool which were marked as relevant by human annotators. Note that each annotator only had access to $\sim 10$ passages during annotation, meaning a pool is likely to contain false negatives, i.e. relevant passages that are not marked as such.

\section{Probing Dataset Generation}
\label{sec:dataset_gen}
For all of our probing tasks, we automatically generate datasets from the TREC2019 passage-level test set. To achieve this, we sample $60$k query-document pairs from the test set of which $40$k are used as training set and $10$k as validation and test set, respectively. We then use existing tools to extract the properties that we are interested in and use them to label the data. In the following we explain our usage of tools for each dataset.

\subsection{BM25 Prediction}
To generate BM25 scores we leverage the Elasticsearch BM25 implementation\footnote{\url{https://www.elastic.co/de/elasticsearch/}}. We first index the full test dataset, then compute bm25 for each query pool, and uniformly sample the $60$k query-document pairs.

\subsection{Named Entity Recognition}
For identifying named entities we use spacy's \cite{spacy2} named entity recognition module. It is capable of detecting and assign one of $18$ different types of entities. We only include pairs that contain at least one entity.


\subsection{Semantic Similarity}
To compute semantic similarity between query and document we first embed all words in the GloVe \cite{pennington2014glove} vector space. We then compute the average embedding for each query and document over the sequence dimension and compute their cosine similarity (\autoref{eq:sem_sim}).

\subsection{Coreference Resolution}
To detect and score entity pairs, we use the neuralcoref pipeline extension for spacy\footnote{\url{https://github.com/huggingface/neuralcoref}}. It consists of a rule-based mentions-detection module, followed by a feed-forward neural network which produces a binary coreference score for each detected pair.

\subsection{Fact Checking}
Fact checking is the only dataset that we don't sample from TREC2019. Instead, we leverage the existing labels of the FEVER fact check dataset \cite{thorne-etal-2018-fever} and sample claim-evidence pairs from the train set.