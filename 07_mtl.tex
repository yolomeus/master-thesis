\chapter{Informed Multi-Task Learning}
Based on our findings from the probing experiments, we now want to address the question on whether the knowledge obtained from probing can be leveraged to improve the BERT model for ranking.

Hence, our knew objective becomes re-ranking: Given a query $q$ and a set of candidate documents $C=\{c_i\}_{i=1}^{|C|} \subseteq D$, we want to learn a ranker $s: Q \times D \rightarrow \R$, such that for any two candidate documents $c, c' \in C$, it holds true that $s(q, c) > s(q, c')$ if $c$ is more relevant than than $c'$, regarding $q$.

During probing we made these key observations:

\begin{itemize}
    \item Most of the probed ranking properties can be decoded from mid layers.
    \item Property are captured around specific layers.
    \item Fine-tuning for ranking can amplify the presence of a property in specific layers.
\end{itemize}

Therefore, we want our experiment to satisfy the following criterions:
\begin{itemize}
    \item Exploit the fact that ranking properties emerge in intermediate layers.
    \item Use the knowledge on which layers capture what property best.
\end{itemize}
The main idea of our approach is to aid fine-tuning, by infusing task knowledge into specific layers, through \ti{multitask learning}. We want to simultaneously learn ranking properties at the layers that we found to be best at capturing them, especially when being fine-tuned for ranking.

To achieve this, for each task we first select a layer that we found to efficiently encode the corresponding  ranking property during probing. We then fine-tune the \ti{bert-base-uncased} model to perform ranking on the TREC2019 dataset. At the same time, we learn classifiers on top of BERT's intermediate layer representations of the pre-selected layers, to predict the respective ranking properties.

\section{Model Architecture}
Given the intermediate layer representations of the pre-trained \ti{bert-base-uncased} model $\{H\lay i\}_{i=0}^{12}$, with $H\lay i = (h_1, h_2, \dots, h_{N})$ being the sequence of token embeddings at layer~$i$, for each task (\autoref{sec:tasks}), we select a layer based on the probing results. We then apply average pooling across the sequence dimension to retrieve a fixed size embedding:

\begin{equation}
    \tx{pool}(h_i, h_{i+1},\dots, h_j)= \frac{1}{j} \sum_{k=i}^j h_k
\end{equation}

In the case of tasks that require multiple spans, we first average along each span $(i,j) \in S$ and then concatenate the resulting vectors:

\begin{equation}
    \tx{multi-pool}(h_1, \dots, h_N) = \bigparallel_{(i, j) \in S}{\tx{pool}(h_i, h_{i+1},\dots, h_j)}
\end{equation}

Following the pooling we apply a simple MLP classifier of the form:

\begin{equation}
    \tx{FFN}(x) = \tx{ReLU}(x W^{(0)} + b^{(0)}) W^{(1)} + b^{(1)}
\end{equation}


\section{Experimental Setup}
\subsection{Datasets and Sampling}
Because our objective is now re-ranking on TREC2019, we can no longer rely on the original probing datasets, as they were sampled from the test set and hence, this would result in test set overlap. Instead, we sample new query-document pairs from the TREC2019 train set. Our sampling goes as follows: We uniformly sample $100$k train queries for each task. Then, for each query we retrieve $10$ documents from the corpus using BM25, resulting in a dataset size of $1$mio samples which is approximately the size of the TREC2019 ranking dataset. Each task dataset is then constructed by applying the same procedure as in \autoref{sec:dataset_gen} and \autoref{sec:tasks}, to automatically generate labels.

We exclude the fact-checking task, as we do not have a straight forward way to automatically generate samples and the dataset itself is too small in comparison to the other datasets.

\subsection{Training}
During training, we assemble mini-batches of size $32$, by sampling from the TREC2019 train set and all generated task datasets with a probability proportional to their size. As loss function we use the objective proposed in \cite{aghajanyan-etal-2021-muppet}:

\begin{equation}
    \mathcal{L}(y, \hat{y}) = \sum_{t \in \tx{tasks}} \frac{\textnormal{CE}(y_t, \hat{y}_t)}{\log c_t}
\end{equation}
Where $c_t$ is the number of target classes and $y_t$, $\hat{y}_t$ are predictions and ground-truth labels with respect to task $t$, respectively. It scales each task loss, such that all losses would have equivalent values, if the class distribution were uniformly distributed, along with the predictions. Analogously to the probing experiments, regression tasks are cast to classification tasks by binning the targets into $k=10$ categories.
We train each model for up to a maximum of $100$ epochs and perform early stopping after $3$ epochs of no improvement in MAP on the TREC2019 validation set. As optimizer, we use Adam\cite{kingma2014adam} with a learning rate of 2e-6 and linearly increase the learning rate over the first $10k$ steps. Each task specific classifier has a hidden size of $128$ and dropout with rate $0.2$ is applied before each layer.

\section{Results}
\begin{table}[!ht]
    \centering
    \begin{tabular}{lc|cccc|c}
        \hline
        \tf{Tasks}  & \tf{Layer} & \tf{MAP}   & \tf{MRR}   & \tf{NDCG@10} & \tf{P@10}  & \tf{avg}   \\ \hline\hline
        \tx{TREC}   & 12         & 0.436      & 0.926      & 0.678        & 0.784      & 0.706      \\ \hline
        \tx{+BM25}  & 5          & 0.437      & 0.947      & 0.682        & \tf{0.791} & 0.714      \\
        ~           & 6          & 0.439      & \tf{0.953} & \tf{0.690}   & 0.772      & 0.714      \\
        ~           & 12         & 0.420      & 0.912      & 0.659        & 0.749      & 0.685      \\ \hline

        \tx{+NER}   & 4          & \tf{0.447} & \tf{0.950} & 0.685        & 0.788      & \tf{0.717} \\
        ~           & 5          & \tf{0.444} & 0.934      & 0.680        & 0.772      & 0.708      \\
        ~           & 12         & \tf{0.447} & 0.944      & \tf{0.688}   & \tf{0.791} & \tf{0.717} \\ \hline
        \tx{+SEM}   & 1          & 0.436      & 0.934      & 0.682        & 0.784      & 0.709      \\
        ~           & 4          & 0.440      & 0.928      & 0.682        & 0.779      & 0.707      \\
        ~           & 12         & 0.436      & 0.928      & 0.669        & 0.774      & 0.702      \\ \hline
        \tx{+COREF} & 5          & 0.442      & \tf{0.965} & \tf{0.694}   & \tf{0.798} & \tf{0.725} \\
        ~           & 7          & 0.425      & 0.944      & 0.668        & 0.770      & 0.702      \\
        ~           & 12         & 0.442      & 0.948      & 0.681        & 0.788      & 0.715      \\
    \end{tabular}
    \caption{Test results for single additional task runs. The first row corresponds to a baseline that was solely trained for ranking on TREC2019. +[TASK] indicates multitask training on TREC2019 and [TASK]. Top 3 runs for each metric are highlighted in bold.}
\end{table}

\section{Ablation}
The prior experiment has shown that explicitly infusing additional task information at specific layers, can result in increased ranking performance. Because of this, we further want to investigate whether this could be a valid augmentation technique in a limited data scenario.

For this, we conduct the following ablation study: Given a subset of TREC2019 query-document pairs, we resample multiple times from this subset and automatically create additional training samples for each of the tasks used in the MTL experiment. We then proceed to train using our proposed multitask method and repeat the experiment for different subset sizes.

