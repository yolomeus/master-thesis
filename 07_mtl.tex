\chapter{Informed Multi-Task Learning}
Based on our findings from the probing experiments, we now want to address the question on whether the knowledge obtained from probing can be leveraged to improve the BERT model for ranking. Hence, we follow a knew objective which is the task of re-ranking.

Given a set of ... ... we want to learn a ranker s ... such that s(q, d) > s(q,d') if more relevant to query ...

During probing we made these key observations:

\begin{itemize}
    \item Most of the probed ranking properties can be decoded from layers earlier than the last.
    \item Each property is mostly captured around specific layers.
    \item Fine-tuning for ranking amplifies the presence of a property in certain layers.
\end{itemize}

Therefore, we design our experiment to satisfy the following criterions:
\begin{itemize}
    \item Exploit the fact that ranking properties emerge in intermediate layers.
    \item Use the knowledge on which layers capture what property best.
\end{itemize}



The main idea of our approach is to aid fine-tuning, by infusing task knowledge into specific layers, through multitask learning. We want to simultaneously learn ranking properties at the layers that we found to be best at capturing them, especially when being fine-tuned for ranking.
To achieve this, we fine-tune the \ti{bert-base-uncased} model to perform ranking on the TREC2019 dataset and at the same time learn classifiers on top of BERT's intermediate layer representations, to predict the ranking properties from our probing tasks.

\section{Model Architecture}
Given the intermediate layer representations of the pre-trained \ti{bert-base-uncased} model $\{H\lay i\}_{i=0}^{12}$, with $H\lay i = (h_1, h_2, \dots, h_{N})$ being the sequence of token embeddings at layer~$i$, for each task (\sect{tasks}), we select a layer based on the probing results. We then apply average pooling across the sequence dimension to retrieve a fixed size embedding:

\begin{equation}
    \tx{pool}(h_i, h_{i+1},\dots, h_j)= \frac{1}{j} \sum_{k=i}^j h_k
\end{equation}

In the case of tasks that require multiple spans, we first average along each span $(i,j) \in S$ and then concatenate the resulting vectors:

\begin{equation}
    \tx{multi-pool}(h_1, \dots, h_N) = \bigparallel_{(i, j) \in S}{\tx{pool}(h_i, h_{i+1},\dots, h_j)}
\end{equation}

Following the pooling we apply a simple MLP classifier of the form:

\begin{equation}
    \tx{FFN}(x) = \tx{ReLU}(x W^{(0)} + b^{(0)}) W^{(1)} + b^{(1)}
\end{equation}


\section{Experimental Setup}
\subsection{Datasets and Sampling}
Because our objective is now ranking on TREC2019, we can no longer rely on the original probing datasets, as they were sampled from the test set and hence this would result in test set overlap. Instead, we sample new query-document pairs from the TREC2019 train set. Our sampling goes as follows: We uniformly sample $100$k train queries for each task. Then, for each query we retrieve $10$ documents from the corpus using BM25, resulting in a dataset size of $1$mio samples which is approximately the size of the TREC2019 ranking dataset. Each task dataset is then constructed by applying the same procedure as in \sect{dataset_gen}, to automatically generate labels.

\todo{fact checking excluded because fever too small, no pipeline for automatic generation}

\subsection{Training}
During training, we assemble mini-batches of size $32$, by sampling from the TREC2019 train set and all generated task datasets with a probability proportional to their size. As loss function we use the objective proposed in \cite{aghajanyan-etal-2021-muppet}:

\begin{equation}
    \mathcal{L}(y, \hat{y}) = \sum_{t \in \tx{tasks}} \frac{\textnormal{CE}(y_t, \hat{y}_t)}{\log c_t}
\end{equation}
Where $c_t$ is the number of target classes and $y_t$, $\hat{y}_t$ are predictions and ground-truth labels with respect to task $t$, respectively. It scales each task loss, such that all losses would have equivalent values, if the class distribution were uniformly distributed, along with the predictions. Analogously to the probing experiments, regression tasks are cast to classification tasks by binning the targets into $k=10$ categories.
We train each model for up to a maximum of $100$ epochs and perform early stopping after $3$ epochs of no improvement in MAP on the TREC2019 validation set. As optimizer, we use Adam\cite{kingma2014adam} with a learning rate of 2e-6 and linearly increase the learning rate over the first $10k$ steps. Each task specific classifier has a hidden size of $128$ and dropout with rate $0.2$ is applied before each layer.

\section{Results}
\section{Ablation}
The prior experiment has shown that explicitly infusing additional task information at specific layers, can result in increased ranking performance. Because of this, we further want to investigate whether this is still a valid augmentation technique in a limited data scenario.

For this, we conduct the following ablation study: Given a subset of TREC2019 query-document pairs, we resample from this subset multiple times and automatically create additional training samples for each of the additional task used in the MTL experiment. We then proceed to train using our proposed multitask method and repeat the experiment for different subset sizes.