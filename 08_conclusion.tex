\chapter{Conclusion}
\label{chap:conclusion}
Throughout this thesis we've explored to what extent different ranking abilities are encoded by a BERT model's word representations. Through probing, we found that different layers are better at encoding certain properties. Whereas lower level layers were better at capturing task knowledge that is helpful for simple, match based query-document relations (SEM, BM25, NER), we found mid- to high-level layers to hold more complex semantic information (COREF, FACT CHECK).

Furthermore, we've investigated how fine-tuning affects the distribution of properties, by probing a version of the model that has been adapted for ranking. We were able to observe a shift in distribution towards mid- and upper-layers and a consistent loss of information in the last layer, regardless of the task. In addition, by comparing absolute compression values between tasks, we found that the overall presence of property information increased through fine-tuning, suggesting that these properties are indeed relevant for learning a ranking model. Interestingly, this effect was especially pronounced with the COREF property.

Then, based on our findings, we've designed a new multi-task learning setup to aid fine-tuning BERT for ranking and found that infusing task information at specific layers can indeed help to learn a better ranking model and that the choice of layer does matter.

For this, we've investigated two ranking objectives, pointwise and pairwise, with different additional task combinations. Through this, we found that for the pointwise objective, providing a single additional task would result in increased ranking performance. However, when increasing the number of tasks the performance would drop. Surprisingly, we could not observe this decrease in performance when using the pairwise ranking objective. Instead, the overall performance did still increase.

\section{Limitations}
\label{sec:limitations}
Firstly, there is a general limitation that arises when using the probing framework. Even though a probe can help us estimate whether a certain property is extractable from a model's representations, we can not conclude that the model itself actually uses that information during inference. While we can gather more evidence on whether a property is important for a downstream task, by comparing to a fine-tuned model, this is still not a guarantee.

Another problem is the quality of automatically generated task data. Because some of our tools used for label generation also rely on learned models, this will certainly result in some wrong predictions, meaning our task labels are to a certain extent noisy, ultimately causing less reliable probing results.

Regarding our MTL experiments, while we could observe a general improvement in ranking performance across the board, this improvement was rather marginal. Whether this is due to the model being too simple, lacking quality in additional task data, or simply because the model already has access to most of the infused information, has yet to be explored.

\section{Future Work}
\label{sec:future}
Considering the limitations of this thesis, we suggest the following directions for future work:
\begin{itemize}
    \item There are still more ranking properties that one might think of. Probing BERT for additional ranking properties can provide further insights on neural ranking.
    \item By using more sophisticated approaches, the automatic label extraction process might be improved, in order to produce higher quality probing datasets.
    \item In this thesis we've only used the TREC2019 dataset. Leveraging additional datasets might give us more general insights and also help with MTL learning.
    \item Since we've used a fairly simple MTL architecture, it might be beneficial to extend our approach with more advanced architectural designs.
\end{itemize}
Whereas this is just a small selection of potential improvements building on this thesis, there is still much more work to be done in understanding the capabilities of neural ranking models.
