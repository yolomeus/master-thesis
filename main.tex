\documentclass[headsepline]{scrreprt}

\usepackage[utf8]{inputenc}
\usepackage{csquotes}
\usepackage[english]{babel}

\usepackage[round]{natbib}
\bibliographystyle{abbrvnat}

\usepackage{scrlayer-scrpage}
\pagestyle{scrheadings}
\automark[section]{section}
\clearpairofpagestyles
\ohead{\headmark}
\ofoot[\pagemark]{\pagemark}

\usepackage{subcaption}

\usepackage[]{graphicx}
\graphicspath{{gfx/}}
\DeclareGraphicsExtensions{.png,.jpg,.pdf}

\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{stmaryrd}
\usepackage{hyperref}
\usepackage{cleveref}

\usepackage[ruled,vlined]{algorithm2e}
\usepackage{todonotes}

% CUSTOM COMMANDS
\newcommand{\handindate}{10.08.2022}
\newcommand{\ti}[1]{\textit{#1}}
\newcommand{\tx}[1]{\text{#1}}
\newcommand{\tf}[1]{\textbf{#1}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\MSubject}{\mathcal{M}}
\newcommand{\lay}[1]{^{(#1)}}

\begin{document}
\input{00_cover.tex}

\chapter*{Abstract}
Nowadays, information retrieval plays an important role in our daily lives. Whether we're searching the web, shopping for products online, or trying to find our favorite movies on a streaming platform: An information retrieval system will be responsible for tackling these tasks. As a consequence of recent advances in natural language processing, employing large pre-trained language models as part of a text retrieval pipeline has become a common approach\footnote{\url{https://searchengineland.com/google-bert-used-on-almost-every-english-query-342193}}. However, despite their proven effectiveness, these neural network based models are functional black boxes, meaning it is not clear to us as to how they arrive at certain decisions. To get a better understanding of the inner workings of such a model, we apply the recently emerging \ti{probing} paradigm. By employing a diagnostic classifier, this approach enables us to analyze how certain properties are encoded within a model's hidden representations. Unlike previous research that has focused on general linguistic properties, we explicitly study the layer-wise distribution of ranking related knowledge throughout the popular BERT model, a large neural network that has been trained on massive amounts of text data. In this thesis, we provide evidence that BERT not only stores ranking related concepts, but also orders them in a hierarchical manner. Moreover, we leverage our findings to design a multi-task learning setup which infuses task specific information at different layers of BERT, in order to improve the model's ability to rank.

\tableofcontents

\input{01_Introduction.tex}
\input{02_foundations.tex}
\input{03_prev_work.tex}
\input{04_data.tex}
\input{05_approach.tex}
\input{06_probing_results.tex}
\input{07_mtl.tex}
\input{08_conclusion.tex}

\chapter*{Plagiarism Statement}
\addcontentsline{toc}{chapter}{Plagiarism Statement}
\vfill
\mbox{} \\
{\large I hereby confirm that this thesis is my own work and that I have documented all sources used. I have not submitted this thesis for another class or module (or any
other means to obtain credit) before.}
\newline
\mbox{} \\
Hannover, \handindate \\
\vspace{4cm}
\hrule
\vspace{0.5cm}
$\qquad$(Fabian Beringer)

\listoffigures
\addcontentsline{toc}{chapter}{\listfigurename}
\listoftables
\addcontentsline{toc}{chapter}{\listtablename}
\clearpage
\phantomsection
\addcontentsline{toc}{chapter}{Bibliography}
\bibliography{bibliography.bib}
\end{document}