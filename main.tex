\documentclass[headsepline]{scrreprt}

\usepackage[utf8]{inputenc}
\usepackage{csquotes}
\usepackage[english]{babel}

\usepackage{scrlayer-scrpage}
\pagestyle{scrheadings}
\automark[section]{section}
\clearpairofpagestyles
\ohead{\headmark}
\ofoot[\pagemark]{\pagemark}

\usepackage{subcaption}

\usepackage[]{graphicx}
\graphicspath{{gfx/}}
\DeclareGraphicsExtensions{.png,.jpg,.pdf}

\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{stmaryrd}
\usepackage{hyperref}
\usepackage{cleveref}

\usepackage[ruled,vlined]{algorithm2e}
\usepackage{todonotes}

% CUSTOM COMMANDS
\newcommand{\handindate}{xx.xx.2022}
\newcommand{\ti}[1]{\textit{#1}}
\newcommand{\tx}[1]{\text{#1}}
\newcommand{\tf}[1]{\textbf{#1}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\MSubject}{\mathcal{M}}
\newcommand{\lay}[1]{^{(#1)}}

\usepackage[backend=biber, style=alphabetic, maxcitenames=2]{biblatex}
\addbibresource{bibliography.bib}

\begin{document}
\input{00_cover.tex}

\chapter*{Abstract}
Nowadays, information retrieval plays an important role in our daily lives. Whether we're searching the web, shopping for products online, or trying to find our favorite movies on a streaming platform: An information retrieval system will be responsible for tackling these tasks. As a consequence of recent advances in natural language processing, employing large pre-trained language models as part of a text retrieval pipeline has become a common approach\footnote{\url{https://searchengineland.com/google-bert-used-on-almost-every-english-query-342193}}. However, despite their proven effectiveness, these neural network based models are functional black boxes, meaning it is not clear to us as to how they arrive at certain decisions. To get a better understanding of the inner workings of such a model, we apply the recently emerging \ti{probing} paradigm. By employing a diagnostic classifier, this approach enables us to analyze how certain properties are encoded within a model's hidden representations. Unlike previous research that has focused on general linguistic properties, we explicitly study the layer-wise distribution of ranking related knowledge throughout the popular BERT model, a large neural network that has been trained on massive amounts of text data. In this thesis, we provide evidence that BERT stores ranking related concepts in a hierarchical manner and leverage our findings to design a multi-task learning setup that further improves the model's ability to rank.

\tableofcontents

\input{01_Introduction.tex}
\input{02_foundations.tex}
\input{03_prev_work.tex}
\input{04_data.tex}
\input{05_approach.tex}
\input{06_probing_results.tex}
\input{07_mtl.tex}
\input{08_conclusion.tex}

\chapter*{Plagiarism Statement}
\addcontentsline{toc}{chapter}{Plagiarism Statement}
\vfill
\mbox{} \\
{\large I hereby confirm that this thesis is my own work and that I have documented all sources used.}
\newline
\mbox{} \\
Hannover, \handindate \\
\vspace{4cm}
\hrule
\vspace{0.5cm}
$\qquad$(Fabian Beringer)

\listoffigures
\addcontentsline{toc}{chapter}{\listfigurename}
\listoftables
\addcontentsline{toc}{chapter}{\listtablename}
\clearpage
\phantomsection
\addcontentsline{toc}{chapter}{Bibliography}
\printbibliography
\end{document}